1
00:00:00,032 --> 00:00:05,077
In this video we are going to introduce a technique called Heuristic Evaluation.

2
00:00:05,077 --> 00:00:11,047
As we talked about at the beginning of the course, there’s lots of different ways to evaluate software.

3
00:00:11,047 --> 00:00:14,058
One that you might be most familiar with is empirical methods,

4
00:00:14,058 --> 00:00:19,045
where, of some level of formality, you have actual people trying out your software.

5
00:00:19,045 --> 00:00:25,029
It’s also possible to have formal methods, where you’re building a model

6
00:00:25,029 --> 00:00:28,019
of how people behave in a particular situation,

7
00:00:28,019 --> 00:00:32,020
and that enables you to predict how different user interfaces will work.

8
00:00:32,020 --> 00:00:36,027
Or, if you can’t build a closed-form formal model,

9
00:00:36,027 --> 00:00:40,046
you can also try out your interface with simulation and have automated tests —

10
00:00:40,046 --> 00:00:44,082
that can detect usability bugs and effective designs.

11
00:00:44,082 --> 00:00:49,075
This works especially well for low-level stuff; it’s harder to do for higher-level stuff.

12
00:00:49,075 --> 00:00:52,093
And what we’re going to tal about today is critique-based approaches,

13
00:00:52,093 --> 00:01:00,005
where people are giving you feedback directly, based on their expertise or a set of heuristics.

14
00:01:00,005 --> 00:01:03,024
As any of you who have ever taken an art or design class know,

15
00:01:03,024 --> 00:01:06,072
peer critique can be an incredibly effective form of feedback,

16
00:01:06,072 --> 00:01:09,024
and it can make you make your designs even better.

17
00:01:09,024 --> 00:01:12,095
You can get peer critique  really at any stage of your design process,

18
00:01:12,095 --> 00:01:16,079
but I’d like to highlight a couple that can be particularly valuable.

17
00:01:18,071 --> 00:01:23,039
First, it's really valuable to get peer critique before user
testing, because that helps not waste your

18
00:01:23,039 --> 00:01:27,079
users on stuff that's just going to get
picked up automatically. You want to be

19
00:01:27,079 --> 00:01:32,025
able to focus the valuable resources of
user testing on stuff that other people

20
00:01:32,025 --> 00:01:36,056
wouldn't be able to pick up on The rich
qualitative feedback that peer critique

21
00:01:36,056 --> 00:01:40,026
provides can also be valuable. Really
valuable before redesigning your

22
00:01:40,026 --> 00:01:44,055
application Because what it can do is it
can show you, what parts of your app you

23
00:01:44,055 --> 00:01:48,041
probably want to keep. And what other
parts that are more problematic and

24
00:01:48,041 --> 00:01:52,083
deserve redesign. Third, sometimes, you
know there are problems And you need data

25
00:01:52,083 --> 00:01:57,060
to be able to convince other stakeholders
to make the changes And peer critique can

26
00:01:57,060 --> 00:02:01,087
be a great way, especially if it's stru
ctured, to be able to get the feedback

27
00:02:01,087 --> 00:02:06,033
that you need, to make the changes that
you now need to happen And lastly, this

28
00:02:06,033 --> 00:02:11,012
kind of structured peer critique can be
really valuable before releasing software

29
00:02:11,012 --> 00:02:15,079
Because it helps you do a final sanding of
the entire design, and smooth out any

30
00:02:15,079 --> 00:02:20,018
rough edges. As with most types of
evaluation, it's usually helpful to begin

31
00:02:20,018 --> 00:02:24,068
with a clear goal. Even if what you
ultimately learn is completely unexpected.

32
00:02:24,093 --> 00:02:29,089
And so, we're gonna talk about today is a
particular technique called Heuristic

33
00:02:29,089 --> 00:02:34,060
Evaluation. Heuristic Evaluation was
created by Jakob Nielsen and colleagues,

34
00:02:34,060 --> 00:02:39,099
about twenty years ago now And the goal of
Heuristic Evaluation is to be able to find

35
00:02:39,099 --> 00:02:45,000
usability problems in the design. I first
learned about heuristic evaluation when I

36
00:02:45,000 --> 00:02:49,086
TA'd James Landay's intro to CI course and
I've been using it and teaching it ever

37
00:02:49,086 --> 00:02:54,049
since. It's a really valuable technique
because it lets you get feedback really

38
00:02:54,049 --> 00:02:59,036
quickly and it's a high bang for the buck
strategy And the slides that I have here

39
00:02:59,036 --> 00:03:04,005
are based off James' slides for this
course And the materials are all available

40
00:03:04,005 --> 00:03:09,024
on Jacob Nielson's website. The basic idea
of heuristic evaluation is that, you're

41
00:03:09,024 --> 00:03:14,091
gonna provide a set of people. Often other
stakeholders on the design team or outside

42
00:03:14,091 --> 00:03:20,059
design experts where the set of heuristics
or principles and they're gonna use those

43
00:03:20,059 --> 00:03:25,086
to look for problems in your design. Each
of them is first gonna do this

44
00:03:25,086 --> 00:03:30,057
independently and so they'll walk through
a variety of tasks using your design to

45
00:03:30,057 --> 00:03:35,061
look for these bugs And you'll see, you
know, that different evaluators are gonna

46
00:03:35,061 --> 00:03:40,022
find different problems And then they're
gonna communicate and talk together only

47
00:03:40,022 --> 00:03:46,069
at the end, afterwards. At the end of the
process, they're gonna get back together

48
00:03:46,069 --> 00:03:51,051
and talk about what they found And this
independent first, gather afterwards is

49
00:03:51,051 --> 00:03:57,014
how you get a wisdom of crowds benefit and
having multiple evaluators And one of the

50
00:03:57,014 --> 00:04:01,018
reasons that we're talking about this
early in the class is that it's a

51
00:04:01,018 --> 00:04:05,067
technique that you can use, either on a
working user interface or on sketches of

52
00:04:05,067 --> 00:04:09,032
user interfaces And so, heuristic
evaluation works really well in

53
00:04:09,032 --> 00:04:13,075
conjunction with paper prototypes and
other rapid, low fidelity techniques that

54
00:04:13,075 --> 00:04:18,074
you may be using to get your design ideas
out quick and fast Here's Neilsen's ten

55
00:04:18,074 --> 00:04:23,059
heuristics, and they're a pretty darn good
set. That said, there's nothing magic

56
00:04:23,059 --> 00:04:28,069
about these heuristics. They do a pretty
good job of covering many of the problems

57
00:04:28,069 --> 00:04:33,073
that you'll see in many user interfaces
But, you can add on any that you want, and

58
00:04:33,073 --> 00:04:39,008
get rid of any that aren't appropriate for
your system. We're going to go over the

59
00:04:39,008 --> 00:04:43,061
content of these ten heuristics in the
next couple lectures And in this lecture

60
00:04:43,061 --> 00:04:48,020
I'd like to introduce the process that
you're going to use with these heuristics.

61
00:04:48,020 --> 00:04:52,089
So here's what you're going to have your
evaluators do. Give them a couple of tasks

62
00:04:52,089 --> 00:04:57,025
to use your design for, and have them do
each task, stepping through carefully

63
00:04:57,025 --> 00:05:00,096
several times. When their doing this
they're, gonna keep the list of usability

64
00:05:00,096 --> 00:05:05,015
principles as a reminder of things to pay
attention to. Now which principles will

65
00:05:05,015 --> 00:05:10,014
you use? I think Nielson's ten heuristics
are a fantastic start, and you can augment

66
00:05:10,014 --> 00:05:14,047
those with anything else that's relevant
for your domain. So, if you have

67
00:05:14,047 --> 00:05:19,035
particular design goals that you would
like your design to achieve, include those

68
00:05:19,035 --> 00:05:24,016
in the list Or if you have particular
goals that you've set up from competitive

69
00:05:24,016 --> 00:05:28,097
analysis of designs that are out there
already, that's great too Or if there are

70
00:05:28,097 --> 00:05:33,067
things that you've seen your or other
designs excel at, those are important

71
00:05:33,067 --> 00:05:39,097
goals too and can be included in your list
of heuristics. And then obviously, the

72
00:05:39,097 --> 00:05:45,014
important part is that you're gonna take
what you learn from these evaluators and

73
00:05:45,014 --> 00:05:50,037
use those violations of the heuristics as
a way of fixing problems and redesigning.

74
00:05:50,037 --> 00:05:55,042
Let's talk a little bit more about why you
might wanna have multiple evaluators

75
00:05:55,042 --> 00:06:00,053
rather than just one. The graph on the
slide is adapted from Jacob Neilsen's work

76
00:06:00,053 --> 00:06:05,045
on heuristic evaluation and what you see
is each black square is a bug that a

77
00:06:05,045 --> 00:06:11,000
particular evaluat or found. An individual
evaluator represents a row of this matrix

78
00:06:11,000 --> 00:06:15,036
and there's about twenty evaluators in
this set. The columns represent the

79
00:06:15,036 --> 00:06:19,096
problems. And what you can see is that
there's some problems that were fine by

80
00:06:19,096 --> 00:06:24,067
relatively few evaluators and other stuff
which almost everybody found. So we're

81
00:06:24,067 --> 00:06:29,056
gonna call the stuff on the right the easy
problems and the stuff on the left hard

82
00:06:29,056 --> 00:06:35,004
problems. And so in aggregate what we can
say is that no evaluator found every

83
00:06:35,004 --> 00:06:40,069
problem, and some evaluators found more
than others And so there are better and

84
00:06:40,069 --> 00:06:46,089
worse people to do this. So why not have
lots of evaluators. Well, as you add more

85
00:06:46,089 --> 00:06:52,006
evaluators they do find more problems But
it kind of tapers off over time. You lose

86
00:06:52,006 --> 00:06:56,081
that benefit eventually And so from a cost
benefit perspective it's just [inaudible]

87
00:06:56,081 --> 00:07:01,050
starts making sense after a certain point.
So where's the peak of this curve? It's of

88
00:07:01,050 --> 00:07:06,011
course gonna depend on the user interface
that you're working with. How much you're

89
00:07:06,011 --> 00:07:10,070
paying people? How much time is involved?
All sorts of factors Jakob Nielsen's rule

90
00:07:10,070 --> 00:07:14,074
of thumb for these kinds of user
interfaces and heuristic evaluation is

91
00:07:14,074 --> 00:07:19,033
that three to five people tends to work
pretty well. And that's been my experience

92
00:07:19,033 --> 00:07:24,015
too. And I think that definitely one of
the reasons that people use heuristic

93
00:07:24,015 --> 00:07:28,042
evaluation Is because it can be an
extremely cost effective way of finding

94
00:07:28,042 --> 00:07:34,021
problems. In one study that Jacob Nielson
ran, he estimated that the cost of the

95
00:07:34,021 --> 00:07:40,031
problems found with heuristic evaluation,
were $500,000 and the cost of performing

96
00:07:40,031 --> 00:07:46,033
it was just over $10,000 And so, he
estimates a 48 fold benefit-cost ratio for

97
00:07:46,033 --> 00:07:52,088
this particular user interface. Obviously
these numbers are back of the envelope and

98
00:07:52,088 --> 00:07:58,014
your mileage will vary. You can think
about how to estimate the benefit that you

99
00:07:58,014 --> 00:08:03,015
get from something like this. If you have
in-house soft ritual that you are using

100
00:08:03,015 --> 00:08:08,040
something like productivity increases that
if you are making an expense reporting

101
00:08:08,040 --> 00:08:13,010
system or other system that will make
peoples time efficiently used, that's a

102
00:08:13,010 --> 00:08:18,024
big usability win And if you got softwa re
that you're making available on the open

103
00:08:18,024 --> 00:08:23,019
market you can think the, the benefit from
sales or other measures like that. One

104
00:08:23,019 --> 00:08:28,035
thing that we can get from that graph is
that evaluators are more likely to find

105
00:08:28,035 --> 00:08:33,031
severe problems and that's good news and
so with a relatively small number of

106
00:08:33,031 --> 00:08:38,034
people you're pretty likely to stumble
across the most important stuff. However

107
00:08:38,034 --> 00:08:43,050
as we saw with just one person in this
particular case, you know even the best

108
00:08:43,050 --> 00:08:48,078
evaluator found only about a third of the
problems of the system And so that's why

109
00:08:48,078 --> 00:08:54,020
ganging up a number of evaluators say five
is gonna get you most of the benefit that

110
00:08:54,020 --> 00:08:59,017
you'll be gonna achieve. If we compare
heuristic evaluation in user testing, one

111
00:08:59,017 --> 00:09:03,088
of the things that we find is that
heuristic evaluation can often be a lot

112
00:09:03,088 --> 00:09:08,096
faster. It takes just an hour to, versus
an evaluator And the mechanics of getting

113
00:09:08,096 --> 00:09:13,073
a user test up and running, can take
longer. Not even accounting for the fact

114
00:09:13,073 --> 00:09:20,021
that you might have to build software
Also, the heuristic evaluation results

115
00:09:20,021 --> 00:09:25,050
Come pre-interpreted because your
evaluators are directly providing you with

116
00:09:25,050 --> 00:09:31,026
problems and things to fix And so it saves
you the time of having to. Infer from the

117
00:09:31,026 --> 00:09:37,007
usability tests what might be the problem
or solution. Now conversely, experts

118
00:09:37,007 --> 00:09:42,033
walking through your system can generate
false positives that wouldn't actually

119
00:09:42,033 --> 00:09:47,059
happen in a real environment And this
indeed does happen and so user testing is

120
00:09:47,059 --> 00:09:53,099
sort of by definition gonna be more
accurate. At the end of the day I think

121
00:09:53,099 --> 00:09:59,025
it's valuable to alternate methods, all of
the different techniques that you'll learn

122
00:09:59,025 --> 00:10:04,014
in cl, in this class for getting feedback,
can each be valuable, and that cycling

123
00:10:04,014 --> 00:10:09,003
through them you can often get the
benefits of each. And that can be because

124
00:10:09,003 --> 00:10:13,073
with user evaluation and user testing,
you'll find different problems, and by

125
00:10:13,073 --> 00:10:19,017
running HE or something like that early in
the design process, you avoid wasting real

126
00:10:19,017 --> 00:10:24,052
users that you may bring in later on. So
now that we've seen the benefits, what are

127
00:10:24,052 --> 00:10:30,009
the steps? The first thing to do is to get
all your evaluators up to sp eed. On what

128
00:10:30,009 --> 00:10:35,060
the, story is behind your software. Any
necessary domain knowledge they might need

129
00:10:35,060 --> 00:10:40,094
And tell them about the scenario that
you're gonna have them step through. Then

130
00:10:40,094 --> 00:10:45,081
obviously, you have the evaluation phase
where people are working through the

131
00:10:45,081 --> 00:10:50,075
interface. Afterwards, each person is
gonna assign a severity rating And you do

132
00:10:50,075 --> 00:10:55,050
this individually first And then you're
gonna aggregate those into a group

133
00:10:55,050 --> 00:11:01,089
severity rating and produce an aggregate
report out of that And finally once you've

134
00:11:01,089 --> 00:11:06,099
got this aggregated report, you can share
that with the design team And the design

135
00:11:06,099 --> 00:11:11,084
team can discuss what to do with that.
Doing this kind of expert review can be

136
00:11:11,084 --> 00:11:16,075
really taxing And so for each of the
scenarios that you lay out in your design,

137
00:11:16,075 --> 00:11:22,056
it can be valuable to have the evaluator
go through that scenario twice. The first

138
00:11:22,056 --> 00:11:28,044
time, they'll just get a sense of it And
the second time; they can focus on more

139
00:11:28,044 --> 00:11:33,048
specific elements. If you've got some walk
up and use system like a, ticket machine

140
00:11:33,048 --> 00:11:38,005
somewhere. Then, you may wanna not give
people any background information at all

141
00:11:38,005 --> 00:11:42,098
Because if you've got people that are just
getting off the bus or the train, and they

142
00:11:42,098 --> 00:11:47,061
walk up to your machine without any prior
information. That's the experience you

143
00:11:47,061 --> 00:11:51,071
want your evaluators' to have. On the
other hand if you're going to have a

144
00:11:51,071 --> 00:11:55,085
genomic system or other expert user
interface, you'll wanna to make sure that

145
00:11:55,085 --> 00:11:59,078
whatever training you would give to real
users, you're gonna give to your

146
00:11:59,078 --> 00:12:03,076
evaluators as well. In other words,
whatever the background is, it should be

147
00:12:03,076 --> 00:12:09,047
realistic. When your evaluators are
walking through your interface, it's going

148
00:12:09,047 --> 00:12:13,094
to be important to produce a list of very
specific problems and explain those

149
00:12:13,094 --> 00:12:18,075
problems with regard to one of the design
[inaudible]. You don't want people to just

150
00:12:18,075 --> 00:12:24,077
to be like I don't like it And in order to
[inaudible] preach you these results for

151
00:12:24,077 --> 00:12:29,064
the design scheme; you'll want to list
each one of these separately so that they

152
00:12:29,064 --> 00:12:34,045
can be dealt with efficiently. Separate
listings can also help you avoid listing

153
00:12:34,045 --> 00:12:39,017
the same repeated proble m over and over
again. If there's a repeated element on

154
00:12:39,035 --> 00:12:44,006
every single screen, you don't want to
list it at every single screen. You wanna

155
00:12:44,006 --> 00:12:48,096
list it once so that it can be fixed once
And these problems can be very detailed,

156
00:12:48,096 --> 00:12:53,091
like the name of something is confusing.
Or it can be something that has to do more

157
00:12:53,091 --> 00:12:58,063
with the flow of the user interface, or
the architecture of the user experience,

158
00:12:58,063 --> 00:13:04,047
and that's not specifically tied to an
interface element. Your evaluator now also

159
00:13:04,047 --> 00:13:09,033
find that something is missing that ought
to be there and this can be sometime

160
00:13:09,033 --> 00:13:14,032
ambiguous with early prototypes like paper
prototypes and so you want to clarify

161
00:13:14,032 --> 00:13:19,011
whether the user interface is something
you believe to be complete or whether

162
00:13:19,011 --> 00:13:23,085
there are intentional elements missing
ahead of time. And of course sometimes

163
00:13:23,085 --> 00:13:28,077
there are features that are going to be
obviously there that are implied by the

164
00:13:28,077 --> 00:13:35,078
user interface and none, so [inaudible]
relax on those. After your evaluators have

165
00:13:35,078 --> 00:13:40,017
gone through the interface, they can each
independently assign a severity rating to

166
00:13:40,017 --> 00:13:44,019
all of the problems that they found And
that's gonna enable you to allocate

167
00:13:44,019 --> 00:13:48,053
resources to fix those problems. It can
also help give you feedback about how well

168
00:13:48,053 --> 00:13:53,003
you're doing in terms of the usability of
your system in general And give you a kind

169
00:13:53,003 --> 00:13:58,026
of a benchmark of your efforts in this
vein. The severity measure that your

170
00:13:58,026 --> 00:14:03,088
evaluators are gonna come up with is gonna
combine several things. It's gonna combine

171
00:14:03,088 --> 00:14:08,084
the frequency, the impact, and the
pervasiveness of the problem that they're

172
00:14:08,084 --> 00:14:14,052
seeing on the screen. So something that is
only in one place may be a less big deal

173
00:14:14,052 --> 00:14:19,055
than something that shows up throughout
the entire user interface. Similarly,

174
00:14:19,055 --> 00:14:24,070
there are going to be some things like
misaligned text, which may be inelegant,

175
00:14:24,070 --> 00:14:30,045
but aren't a deal killer in terms of your
software And here is the severity rating

176
00:14:30,045 --> 00:14:35,026
system that Nielsen created. You can
obviously use anything you want. It ranges

177
00:14:35,026 --> 00:14:40,019
from zero to four where zero is at the end
of the day your evaluator decides it

178
00:14:40,019 --> 00:14:45,013
actually is not usability problem... All
the way up to it being something really

179
00:14:45,013 --> 00:14:50,089
catas... Catastrophic that has to be fixed
right away And here is an example of a

180
00:14:50,089 --> 00:14:56,027
particular problem that RTA Robby found
when he was taking CS147 as a student. He

181
00:14:56,027 --> 00:15:01,079
walked into somebody's mobile interface
that had a wait entry element to it and he

182
00:15:01,079 --> 00:15:07,024
realized that once you entered your way,
there is no way to edit it after the fact.

183
00:15:07,024 --> 00:15:12,073
So That's kinda clunky, you wish you could
fix it, maybe not a disaster And so what

184
00:15:12,073 --> 00:15:17,085
you see here is he's listed the issue,
he's given it a severity rating, he's got

185
00:15:17,085 --> 00:15:24,019
the heuristic that it violates and then he
describes exactly what the problem is And

186
00:15:24,019 --> 00:15:28,038
finally, after all your evaluators have
gone through the interface, listed their

187
00:15:28,038 --> 00:15:32,057
problems, and combined them in terms of
the severity and importance, you'll want

188
00:15:32,057 --> 00:15:36,060
to debrief with the design team. This is a
nice chance to be able to discuss

189
00:15:36,060 --> 00:15:40,063
generalist use in the interface and
qualitative feedback, and it gives you a

190
00:15:40,063 --> 00:15:44,098
chance to go through each of these line
items and suggest improvements on how you

191
00:15:44,098 --> 00:15:50,050
can address these problems. In this
debrief session, it can be valuable for

192
00:15:50,050 --> 00:15:55,021
the development team to estimate the
amount of effort that it would take to fix

193
00:15:55,021 --> 00:15:59,098
one of these problems So, for example, if
you've got something that is one on your

194
00:15:59,098 --> 00:16:04,051
severity scale, not too big a deal. It
might have something to do with wording

195
00:16:04,051 --> 00:16:09,012
And its dirt simple to fix. That tells you
go ahead and fix it. Conversely, you may

196
00:16:09,012 --> 00:16:13,039
having something which is a catastrophe
which takes a lot more effort, but its

197
00:16:13,039 --> 00:16:17,083
importance will lead you to fix it And
there's other things where the importance

198
00:16:17,083 --> 00:16:22,044
relative to the cost involved, just don't
make sense to deal with right now And this

199
00:16:22,044 --> 00:16:26,082
debrief session can be a great way to
brainstorm future design ideas, especially

200
00:16:26,082 --> 00:16:31,015
while you've got all the stakeholders in
the room, and the ideas about what the

201
00:16:31,015 --> 00:16:35,035
issues are with the user interface are
fresh in their mind. In the next two

202
00:16:35,035 --> 00:16:40,011
videos we'll go through Neilsons' ten
heuristics and talk more about what they

1
00:00:00,032 --> 00:00:05,077
In this video we are going to introduce a technique called Heuristic Evaluation.

2
00:00:05,077 --> 00:00:11,047
As we talked about at the beginning of the course, there’s lots of different ways to evaluate software.

3
00:00:11,047 --> 00:00:14,058
One that you might be most familiar with is empirical methods,

4
00:00:14,058 --> 00:00:19,045
where, of some level of formality, you have actual people trying out your software.

5
00:00:19,045 --> 00:00:25,029
It’s also possible to have formal methods, where you’re building a model

6
00:00:25,029 --> 00:00:28,019
of how people behave in a particular situation,

7
00:00:28,019 --> 00:00:32,020
and that enables you to predict how different user interfaces will work.

8
00:00:32,020 --> 00:00:36,027
Or, if you can’t build a closed-form formal model,

9
00:00:36,027 --> 00:00:40,046
you can also try out your interface with simulation and have automated tests —

10
00:00:40,046 --> 00:00:44,082
that can detect usability bugs and effective designs.

11
00:00:44,082 --> 00:00:49,075
This works especially well for low-level stuff; it’s harder to do for higher-level stuff.

12
00:00:49,075 --> 00:00:52,093
And what we’re going to talk about today is critique-based approaches,

13
00:00:52,093 --> 00:01:00,005
where people are giving you feedback directly, based on their expertise or a set of heuristics.

14
00:01:00,005 --> 00:01:03,024
As any of you who have ever taken an art or design class know,

15
00:01:03,024 --> 00:01:06,072
peer critique can be an incredibly effective form of feedback,

16
00:01:06,072 --> 00:01:09,024
and it can make you make your designs even better.

17
00:01:09,024 --> 00:01:12,095
You can get peer critique really at any stage of your design process,

18
00:01:12,095 --> 00:01:16,079
but I’d like to highlight a couple that I think can be particularly valuable.

17
00:01:18,071 --> 00:01:23,039
First, it’s really valuable to get peer critique before user testing,

18
00:01:23,039 --> 00:01:27,079
because that helps you not waste your users on stuff that’s just going to get picked up automatically.

19
00:01:27,079 --> 00:01:32,025
You want to be able to focus the valuable resources of user testing on stuff that other people wouldn’t be able to pick up on.

20
00:01:32,025 --> 00:01:36,056
The rich qualitative feedback that peer critique provides

21
00:01:36,056 --> 00:01:40,026
can also be really valuable before redesigning your application,

22
00:01:40,026 --> 00:01:44,055
because what it can do is it can show you what parts of your app you probably want to keep,

23
00:01:44,055 --> 00:01:48,041
and what other parts are more problematic and deserve redesign.

24
00:01:48,041 --> 00:01:52,083
Third, sometimes, you know there are problems,

25
00:01:52,083 --> 00:01:57,060
and you need data to be able to convince other stakeholders to make the changes.

26
00:01:57,060 --> 00:02:01,087
And peer critique can be a great way, especially if it’s structured,

27
00:02:01,087 --> 00:02:06,033
to be able to get the feedback that you need, to make the changes that you know need to happen.

28
00:02:06,033 --> 00:02:11,012
And lastly, this kind of structured peer critique can be really valuable before releasing software,

29
00:02:11,012 --> 00:02:15,079
because it helps you do a final sanding of the entire design, and smooth out any rough edges.

30
00:02:15,079 --> 00:02:20,018
As with most types of evaluation, it’s usually helpful to begin with a clear goal,

31
00:02:20,018 --> 00:02:24,068
even if what you ultimately learn is completely unexpected.

32
00:02:24,093 --> 00:02:29,089
And so, what we’re going to talk about today is a particular technique called Heuristic Evaluation.

33
00:02:29,089 --> 00:02:34,060
Heuristic Evaluation was created by Jakob Nielsen and colleagues, about twenty years ago now.

34
00:02:34,060 --> 00:02:39,099
And the goal of Heuristic Evaluation is to be able to find usability problems in the design.

35
00:02:39,099 --> 00:02:45,000
I first learned about Heuristic Evaluation

36
00:02:45,000 --> 00:02:49,086
when I TA’d James Landay’s Intro to HCI course, and I’ve been using it and teaching it ever since.

37
00:02:49,086 --> 00:02:54,049
It’s a really valuable technique because it lets you get feedback really quickly

38
00:02:54,049 --> 00:02:59,036
and it’s a high bang-for-the-buck strategy.

39
00:02:59,036 --> 00:03:04,005
And the slides that I have here are based off James’ slides for this course, and the materials are all available on Jacob Nielsen’s website.

40
00:03:04,005 --> 00:03:09,024
The basic idea of heuristic evaluation is that you’re going to provide a set of people —

41
00:03:09,024 --> 00:03:14,091
often other stakeholders on the design team or outside design experts — with a set of heuristics or principles,

42
00:03:14,091 --> 00:03:20,059
and they’re going to use those to look for problems in your design.

43
00:03:20,059 --> 00:03:25,086
Each of them is first going to do this independently

44
00:03:25,086 --> 00:03:30,057
and so they’ll walk through a variety of tasks using your design to look for these bugs.

45
00:03:30,057 --> 00:03:35,061
And you’ll see, you know, that different evaluators are going to find different problems.

46
00:03:35,061 --> 00:03:40,022
And then they’re going to communicate and talk together only at the end, afterwards.

47
00:03:40,022 --> 00:03:46,069
At the end of the process, they’re going to get back together and talk about what they found.

48
00:03:46,069 --> 00:03:51,051
And this “independent first, gather afterwards”

49
00:03:51,051 --> 00:03:57,014
is how you get a wisdom of crowds benefit in having multiple evaluators.

50
00:03:57,014 --> 00:04:01,018
And one of the reasons that we’re talking about this early in the class

51
00:04:01,018 --> 00:04:05,067
is that it’s a technique that you can use, either on a working user interface or on sketches of user interfaces.

52
00:04:05,067 --> 00:04:09,032
And so heuristic evaluation works really well in conjunction with paper prototypes

53
00:04:09,032 --> 00:04:13,075
and other rapid, low fidelity techniques that you may be using to get your design ideas out quick and fast.

54
00:04:13,075 --> 00:04:18,074
Here’s Neilsen’s ten heuristics, and they’re a pretty darn good set.

55
00:04:18,074 --> 00:04:23,059
That said, there’s nothing magic about these heuristics.

56
00:04:23,059 --> 00:04:28,069
They do a pretty good job of covering many of the problems that you’ll see in many user interfaces;

57
00:04:28,069 --> 00:04:33,073
but you can add on any that you want

58
00:04:33,073 --> 00:04:39,008
and get rid of any that aren’t appropriate for your system.

59
00:04:39,008 --> 00:04:43,061
We’re going to go over the content of these ten heuristics in the next couple lectures,

60
00:04:43,061 --> 00:04:48,020
and in this lecture I’d like to introduce the process that you’re going to use with these heuristics.

61
00:04:48,020 --> 00:04:52,089
So here’s what you’re going to have your evaluators do:

62
00:04:52,089 --> 00:04:57,025
Give them a couple of tasks to use your design for, and have them do each task, stepping through carefully several times.

63
00:04:57,025 --> 00:05:00,096
When they’re doing this they’re going to keep the list of usability principles as a reminder of things to pay attention to.

64
00:05:00,096 --> 00:05:05,015
Now which principles will you use?

65
00:05:05,015 --> 00:05:10,014
I think Nielsen’s ten heuristics are a fantastic start,

66
00:05:10,014 --> 00:05:14,047
and you can augment those with anything else that’s relevant for your domain.

67
00:05:14,047 --> 00:05:19,035
So, if you have particular design goals that you would like your design to achieve, include those in the list.

68
00:05:19,035 --> 00:05:24,016
Or, if you have particular goals that you’ve set up from competitive analysis of designs that are out there already,

69
00:05:24,016 --> 00:05:28,097
that’s great too.

70
00:05:28,097 --> 00:05:33,067
Or if there are things that you’ve seen your or other designs excel at,

71
00:05:33,067 --> 00:05:39,097
those are important goals too and can be included in your list of heuristics.

72
00:05:39,097 --> 00:05:45,014
And then obviously, the important part is that you’re going to take what you learn from these evaluators

73
00:05:45,014 --> 00:05:50,037
and use those violations of the heuristics as a way of fixing problems and redesigning.

74
00:05:50,037 --> 00:05:55,042
Let’s talk a little bit more about why you might want to have multiple evaluators rather than just one.

75
00:05:55,042 --> 00:06:00,053
The graph on the slide is adapted from Jacob Neilsen’s work on heuristic evaluation

76
00:06:00,053 --> 00:06:05,045
and what you see is each black square is a bug that a particular evaluator found.

77
00:06:05,045 --> 00:06:11,000
An individual evaluator represents a row of this matrix

78
00:06:11,000 --> 00:06:15,036
and there’s about twenty evaluators in this set.

79
00:06:15,036 --> 00:06:19,096
The columns represent the problems.

80
00:06:19,096 --> 00:06:24,067
And what you can see is that there’s some problems that were fine by relatively few evaluators and other stuff which almost everybody found.

81
00:06:24,067 --> 00:06:29,056
So we’re going to call the stuff on the right the easy problems and the stuff on the left hard problems.

82
00:06:29,056 --> 00:06:35,004
And so, in aggregate, what we can say is that no evaluator found every problem,

83
00:06:35,004 --> 00:06:40,069
and some evaluators found more than others, and so there are better and worse people to do this.

84
00:06:40,069 --> 00:06:46,089
So why not have lots of evaluators?

85
00:06:46,089 --> 00:06:52,006
Well, as you add more evaluators, they do find more problems; but it kind of tapers off over time — you lose that benefit eventually.

86
00:06:52,006 --> 00:06:56,081
And so from a cost-benefit perspective it’s just stops making sense after a certain point.

87
00:06:56,081 --> 00:07:01,050
So where’s the peak of this curve?

88
00:07:01,050 --> 00:07:06,011
It’s of course going to depend on the user interface that you’re working with,

89
00:07:06,011 --> 00:07:10,070
how much you’re paying people, how much time is involved — all sorts of factors.

90
00:07:10,070 --> 00:07:14,074
Jakob Nielsen’s rule of thumb for these kinds of user interfaces and heuristic evaluation

91
00:07:14,074 --> 00:07:19,033
is that three to five people tends to work pretty well; and that’s been my experience too.

92
00:07:19,033 --> 00:07:24,015
And I think that definitely one of the reasons that people use heuristic evaluation

93
00:07:24,015 --> 00:07:28,042
is because it can be an extremely cost-effective way of finding problems.

94
00:07:28,042 --> 00:07:34,021
In one study that Jacob Nielsen ran,

95
00:07:34,021 --> 00:07:40,031
he estimated that the cost of the problems found with heuristic evaluation

96
00:07:40,031 --> 00:07:46,033
were $500,000 and the cost of performing it was just over $10,000,

97
00:07:46,033 --> 00:07:52,088
and so he estimates a 48-fold benefit-cost ratio for this particular user interface.

98
00:07:52,088 --> 00:07:58,014
Obviously, these numbers are back of the envelope and your mileage will vary.

99
00:07:58,014 --> 00:08:03,015
You can think about how to estimate the benefit that you get from something like this

100
00:08:03,015 --> 00:08:08,040
if you have in-house software tool using something like productivity increases —

101
00:08:08,040 --> 00:08:13,010
that, if you are making an expense reporting system or other system that will make peoples time efficiently used —

102
00:08:13,010 --> 00:08:18,024
that’s a big usability win.

103
00:08:18,024 --> 00:08:23,019
And if you’ve got software that you’re making available on the open market, you can think about the benefit from sales or other measures like that.

104
00:08:23,019 --> 00:08:28,035
One thing that we can get from that graph is that evaluators are more likely to find severe problems and that’s good news

105
00:08:28,035 --> 00:08:33,031
and so with a relatively small number of people,

106
00:08:33,031 --> 00:08:38,034
you’re pretty likely to stumble across the most important stuff.

107
00:08:38,034 --> 00:08:43,050
However, as we saw with just one person in this particular case,

108
00:08:43,050 --> 00:08:48,078
even the best evaluator found only about a third of the problems of the system.

109
00:08:48,078 --> 00:08:54,020
And so that’s why ganging up a number of evaluators, say five, is going to get you most of the benefit that you’ll be going to achieve.

110
00:08:54,020 --> 00:08:59,017
If we compare heuristic evaluation in user testing,

111
00:08:59,017 --> 00:09:03,088
one of the things that we find is that heuristic evaluation can often be a lot faster —

112
00:09:03,088 --> 00:09:08,096
It takes just an hour or two for an evaluator.

113
00:09:08,096 --> 00:09:13,073
And the mechanics of getting a user test up and running can take longer,

114
00:09:13,073 --> 00:09:20,021
not even accounting for the fact that you might have to build software.

115
00:09:20,021 --> 00:09:25,050
Also, the heuristic evaluation results come pre-interpreted

116
00:09:25,050 --> 00:09:31,026
because your evaluators are directly providing you with problems and things to fix,

117
00:09:31,026 --> 00:09:37,007
and so it saves you the time of having to infer from the usability tests what might be the problem or solution.

118
00:09:37,007 --> 00:09:42,033
Now conversely, experts walking through your system can generate false positives that wouldn’t actually happen in a real environment.

119
00:09:42,033 --> 00:09:47,059
And this indeed does happen,

120
00:09:47,059 --> 00:09:53,099
and so user testing is, sort of, by definition going to be more accurate.

121
00:09:53,099 --> 00:09:59,025
At the end of the day I think it’s valuable to alternate methods:

122
00:09:59,025 --> 00:10:04,014
All of the different techniques that you’ll learn in this class for getting feedback can each be valuable,

123
00:10:04,014 --> 00:10:09,003
and that [by] cycling through them you can often get the benefits of each.

124
00:10:09,003 --> 00:10:13,073
And that can be because with user evaluation and user testing, you’ll find different problems,

125
00:10:13,073 --> 00:10:19,017
and by running HE or something like that early in the design process, you’ll avoid wasting real users that you may bring in later on.

126
00:10:19,017 --> 00:10:24,052
So now that we’ve seen the benefits, what are the steps?

127
00:10:24,052 --> 00:10:30,009
The first thing to do is to get all of your evaluators up to speed,

128
00:10:30,009 --> 00:10:35,060
on what the story is behind your software — any necessary domain knowledge they might need —

129
00:10:35,060 --> 00:10:40,094
and tell them about the scenario that you’re going to have them step through.

130
00:10:40,094 --> 00:10:45,081
Then obviously, you have the evaluation phase where people are working through the interface.

131
00:10:45,081 --> 00:10:50,075
Afterwards, each person is going to assign a severity rating,

132
00:10:50,075 --> 00:10:55,050
and you do this individually first,

133
00:10:55,050 --> 00:11:01,089
and then you’re going to aggregate those into a group severity rating and produce an aggregate report out of that.

134
00:11:01,089 --> 00:11:06,099
And finally, once you’ve got this aggregated report, you can share that with the design team,

135
00:11:06,099 --> 00:11:11,084
and the design team can discuss what to do with that.

136
00:11:11,084 --> 00:11:16,075
Doing this kind of expert review can be really taxing,

137
00:11:16,075 --> 00:11:22,056
and so for each of the scenarios that you lay out in your design, it can be valuable to have the evaluator go through that scenario twice.

138
00:11:22,056 --> 00:11:28,044
The first time, they’ll just get a sense of it; and the second time, they can focus on more specific elements.

139
00:11:28,044 --> 00:11:33,048
If you’ve got some walk-up-and-use system, like a ticket machine somewhere,

140
00:11:33,048 --> 00:11:38,005
then you may want to not give people any background information at all,

141
00:11:38,005 --> 00:11:42,098
because if you’ve got people that are just getting off the bus or the train,

142
00:11:42,098 --> 00:11:47,061
and they walk up to your machine without any prior information,

143
00:11:47,061 --> 00:11:51,071
that’s the experience you want your evaluators’ to have.

144
00:11:51,071 --> 00:11:55,085
On the other hand, if you’re going to have a genomic system or other expert user interface,

145
00:11:55,085 --> 00:11:59,078
you’ll want to to make sure that whatever training you would give to real users, you’re going to give to your evaluators as well.

146
00:11:59,078 --> 00:12:03,076
In other words, whatever the background is, it should be realistic.

147
00:12:03,076 --> 00:12:09,047
When your evaluators are walking through your interface,

148
00:12:09,047 --> 00:12:13,094
it’s going to be important to produce a list of very specific problems

149
00:12:13,094 --> 00:12:18,075
and explain those problems with regard to one of the design heuristics.

150
00:12:18,075 --> 00:12:24,077
You don’t want people to just to be, like, “I don’t like it.”

151
00:12:24,077 --> 00:12:29,064
And in order to {maxilinearly preach} you these results for the design scheme;

152
00:12:29,064 --> 00:12:34,045
you’ll want to list each one of these separately so that they can be dealt with efficiently.

153
00:12:34,045 --> 00:12:39,017
Separate listings can also help you avoid listing the same repeated problem over and over again.

154
00:12:39,035 --> 00:12:44,006
If there’s a repeated element on every single screen, you don’t want to list it at every single screen;

155
00:12:44,006 --> 00:12:48,096
you want to list it once so that it can be fixed once.

156
00:12:48,096 --> 00:12:53,091
And these problems can be very detailed, like “the name of something is confusing,”

157
00:12:53,091 --> 00:12:58,063
or it can be something that has to do more with the flow of the user interface,

158
00:12:58,063 --> 00:13:04,047
or the architecture of the user experience and that’s not specifically tied to an interface element.

159
00:13:04,047 --> 00:13:09,033
Your evaluators may also find that something is missing that ought to be there,

160
00:13:09,033 --> 00:13:14,032
and this can be sometime ambiguous with early prototypes, like paper prototypes.

161
00:13:14,032 --> 00:13:19,011
And so you want to clarify whether the user interface is something you believe to be complete,

162
00:13:19,011 --> 00:13:23,085
or whether there are intentional elements missing ahead of time.

163
00:13:23,085 --> 00:13:28,077
And, of course, sometimes there are features that are going to be obviously there that are implied by the user interface.

164
00:13:28,077 --> 00:13:35,078
And, so [inaudible] relax on those.

165
00:13:35,078 --> 00:13:40,017
After your evaluators have gone through the interface,

166
00:13:40,017 --> 00:13:44,019
they can each independently assign a severity rating to all of the problems that they found.

167
00:13:44,019 --> 00:13:48,053
And that’s going to enable you to allocate resources to fix those problems.

168
00:13:48,053 --> 00:13:53,003
It can also help give you feedback about how well you’re doing in terms of the usability of your system in general,

169
00:13:53,003 --> 00:13:58,026
and give you a kind of a benchmark of your efforts in this vein.

170
00:13:58,026 --> 00:14:03,088
The severity measure that your evaluators are going to come up with is going to combine several things:

171
00:14:03,088 --> 00:14:08,084
It’s going to combine the frequency, the impact, and the pervasiveness of the problem that they’re seeing on the screen.

172
00:14:08,084 --> 00:14:14,052
So, something that is only in one place may be a less big deal

173
00:14:14,052 --> 00:14:19,055
than something that shows up throughout the entire user interface.

174
00:14:19,055 --> 00:14:24,070
Similarly, there are going to be some things like misaligned text,

175
00:14:24,070 --> 00:14:30,045
which may be inelegant, but aren’t a deal killer in terms of your software.

176
00:14:30,045 --> 00:14:35,026
And here is the severity rating system that Nielsen created; you can obviously use anything that you want:

177
00:14:35,026 --> 00:14:40,019
It ranges from zero to four, where zero is at the end of the day your evaluator decides it actually is not usability problem,

178
00:14:40,019 --> 00:14:45,013
all the way up to it being something really catastrophic

179
00:14:45,013 --> 00:14:50,089
that has to get fixed right away.

180
00:14:50,089 --> 00:14:56,027
And here is an example of a particular problem that our TA Robby found when he was taking CS147 as a student.

181
00:14:56,027 --> 00:15:01,079
He walked through somebody’s mobile interface that had a “weight” entry element to it;

182
00:15:01,079 --> 00:15:07,024
and he realized that once you entered your weight, there is no way to edit it after the fact.

183
00:15:07,024 --> 00:15:12,073
So, that’s kinda clunky — you wish you could fix it; maybe not a disaster.

184
00:15:12,073 --> 00:15:17,085
And so what you see here is he’s listed the issue, he’s given it a severity rating,

185
00:15:17,085 --> 00:15:24,019
he’s got the heuristic that it violates, and then he describes exactly what the problem is.

186
00:15:24,019 --> 00:15:28,038
And finally, after all your evaluators have gone through the interface,

187
00:15:28,038 --> 00:15:32,057
listed their problems, and combined them in terms of the severity and importance,

188
00:15:32,057 --> 00:15:36,060
you’ll want to debrief with the design team.

189
00:15:36,060 --> 00:15:40,063
This is a nice chance to be able to discuss general issues in the user interface and qualitative feedback,

190
00:15:40,063 --> 00:15:44,098
and it gives you a chance to go through each of these items and suggest improvements on how you can address these problems.

191
00:15:44,098 --> 00:15:50,050
In this debrief session,

192
00:15:50,050 --> 00:15:55,021
it can be valuable for the development team to estimate the amount of effort that it would take to fix one of these problems.

193
00:15:55,021 --> 00:15:59,098
So, for example, if you’ve got something that is one on your severity scale —

194
00:15:59,098 --> 00:16:04,051
not too big a deal; it might have something to do with wording and its dirt simple to fix —

195
00:16:04,051 --> 00:16:09,012
that tells you “go ahead and fix it.”

196
00:16:09,012 --> 00:16:13,039
Conversely, you may having something which is a catastrophe

197
00:16:13,039 --> 00:16:17,083
which takes a lot more effort, but its importance will lead you to fix it.

198
00:16:17,083 --> 00:16:22,044
And there’s other things where the importance relative to the cost involved just don’t make sense to deal with right now.

199
00:16:22,044 --> 00:16:26,082
And this debrief session can be a great way to brainstorm future design ideas,

200
00:16:26,082 --> 00:16:31,015
especially while you’ve got all the stakeholders in the room,

201
00:16:31,015 --> 00:16:35,035
and the ideas about what the issues are with the user interface are fresh in their mind.

202
00:16:35,035 --> 00:16:40,011
In the next two videos we’ll go through Neilsons’ ten heuristics and talk more about what they mean.

In this video we’re going to talk about running experiments online.
The web has offered tremendous power
in terms of being able to do experimental work,
roll out different versions of the user interface,
get feedback, and iterate quickly.
You may have heard of this under a bunch of different names:
Sometimes it is called A/B test,
or randomized experiments online,
or controlled experiments,
or split testing.
In all cases the basic idea is the same:
What you’re going to do is you’re going to
randomly split the traffic that comes to your website
between two or more versions.
So, when we talk about it as A/B testing
you’ve got your A version — which is the current live version, usually —
and you’ve got your B version —
which is usually something new that you’re trying.
And what you’re going to do
is collect metrics about how the two versions perform —
in terms of conversions, or clickthroughs,
or other things that you can measure —
and then analyze that afterwards to decide
which of your designs is more effective.
Let’s start out with an example.

Here’s a website for the National Alert Registry,
which provides information on sexual offenders.
This is the first version of the site.
Here is a second version.
And here is a third version.
In this third version it was changed to a two-column format,
and so the intuition behind that was to see
whether gettting more content above the fold on the first screen would help.
Before I show you the results,
think about which of these three user interfaces
you believe [is] the most effective
and how you might measure which of them you think works best.

You can see all three of them together.
Now what you can see is that
this third version — the two-column version —
although it was *intended* to be more much more effective —
it was a redesigned version that people hope would improve things —
what you see is that the number of sales dropped dramatically,
by more than half.
And I think this is really important,
because even the very best designers often,
when they make a revision, it gets worse.
And what the web enables us to do for the first time
is make it easy to try out something new
to get real feedback from real people
about whether that has become [an] effective change or not.

One thing that you’ll notice is that
each of three versions got the same amount of traffic.
So why does this redesigned version do so much worse?

I think there are a couple of theories that you can draw.
One of them is on the top of the screen there is a big “Free.”
And so if our metric is going to be sales,
a screen that has a “Free” up top may dissuade people from buying something.
Another one is, by creating two columns, there may be less of a clear flow,
and so people see a menu of possible options,
when what they really want — if your goal is to have increased checkouts —
is a stronger funnel to push people through the process.

By contrast, version B which does best
doesn’t have this “Free” up top
and it has a much stronger call to action.

So what we see in this simple example from A List Apart
is a number of important ways that small changes in design
can make a significant difference in the effectiveness of a site.

As A List Apart writes, these can include things like:
The position and colour of the primary call to action —
what are you conveying to users that they should do on this page?
Where are testimonials used?
And how are links and images conveyed on the page?
How much whitespace does a page have?
Where is the main heading and how prominent is it?
How many columns does a page use?
What are the visual elements that are competiting for somebody’s attention?
And what’s going on in the photos?
In our visual design lectures
we talked about strategies for using these tools more effectively;
In this lecture we’re going to look at techniques
for being able to measure whether whether the outcome is the outcome you like.

Here’s another example;
this one’s from Forrest Glick at the Stanford Technology Ventures Program.
They have a regular email list
where they send out links to videos about entrepreneurship that they record.
It’s a great mailing list,
and Forrest was interested in increasing the number of people
who click through this email to see the content that they have on offer.

This is the original version of their email,
and here’s a revised version.
In this case the change is pretty subtle:
The goal is to provide more real estate and emphasis
to the Quick Shots at the top of the email.
This email was sent to a little more than 12,000 people,
about half to each.
And what you see is modest changes
in the number of people who look at the full content —
that is, the email client loaded the external resources —,
the number of people who clicked through one of these links to web content,
and a reasonable significant increase
in the number of people who forwarded this email to somebody else.
What I like about this email example
is it show how you can use these techniques
not just for things that are literally web sites,
but any time that you have online media,
you can get this kind of feedback.
And it also shows how you don’t need to make drastic changes;
you can make relatively small changes and still see significant results.

In 2008, Stanford alumni Dan Siroker left his job at Google
to join the Obama campaign.
And what Dan worked on with the Obama campaign
was improving the effectiveness of their online materials.
After the campaign,
we invited Dan to come back and speak to Stanford students
about the techniques that they were using.
In this video Dan describes a couple of experiments that they did.
Think about of the multiple different versions that you’re going to see:
Which one *you* think would be the most effective —
if you were in the driver’s seat, which one would you pick?

([Voice of Dan Siroker] And here’s an experiment that we did:
This is sort of the layout.
We did a multivariate test,
which is actually just a big complicated word to describe
“trying variations of two different parts of the page.”
We did different kinds of media —
so the top part of the page.
And we tried different kinds of buttons.
A baseline signup rate was 8.26%.
So these are the different buttons we used and the different media.
We had four different buttons:
“Sign Up,” “Learn More,” “Join Us Now,” “Sign Up Now,”
and six different media — three images and three videos.


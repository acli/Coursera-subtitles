In this video we’re going to talk about running experiments online.
The web has offered tremendous power
in terms of being able to do experimental work,
roll out different versions of the user interface,
get feedback, and iterate quickly.
You may have heard of this under a bunch of different names:
Sometimes it is called A/B test,
or randomized experiments online,
or controlled experiments,
or split testing.
In all cases the basic idea is the same:
What you’re going to do is you’re going to
randomly split the traffic that comes to your website
between two or more versions.
So, when we talk about it as A/B testing
you’ve got your A version — which is the current live version, usually —
and you’ve got your B version —
which is usually something new that you’re trying.
And what you’re going to do
is collect metrics about how the two versions perform —
in terms of conversions, or clickthroughs,
or other things that you can measure —
and then analyze that afterwards to decide
which of your designs is more effective.
Let’s start out with an example.

Here’s a website for the National Alert Registry,
which provides information on sexual offenders.
This is the first version of the site.
Here is a second version.
And here is a third version.
In this third version it was changed to a two-column format,
and so the intuition behind that was to see
whether gettting more content above the fold on the first screen would help.
Before I show you the results,
think about which of these three user interfaces
you believe [is] the most effective
and how you might measure which of them you think works best.

You can see all three of them together.
Now what you can see is that
this third version — the two-column version —
although it was *intended* to be more much more effective —
it was a redesigned version that people hope would improve things —
what you see is that the number of sales dropped dramatically,
by more than half.
And I think this is really important,
because even the very best designers often,
when they make a revision, it gets worse.
And what the web enables us to do for the first time
is make it easy to try out something new
to get real feedback from real people
about whether that has become [an] effective change or not.

One thing that you’ll notice is that
each of three versions got the same amount of traffic.
So why does this redesigned version do so much worse?

I think there are a couple of theories that you can draw.
One of them is on the top of the screen there is a big “Free.”
And so if our metric is going to be sales,
a screen that has a “Free” up top may dissuade people from buying something.
Another one is, by creating two columns, there may be less of a clear flow,
and so people see a menu of possible options,
when what they really want — if your goal is to have increased checkouts —
is a stronger funnel to push people through the process.

By contrast, version B which does best
doesn’t have this “Free” up top
and it has a much stronger call to action.

So what we see in this simple example from A List Apart
is a number of important ways that small changes in design
can make a significant difference in the effectiveness of a site.

As A List Apart writes, these can include things like:
The position and colour of the primary call to action —
what are you conveying to users that they should do on this page?
Where are testimonials used?
And how are links and images conveyed on the page?
How much whitespace does a page have?
Where is the main heading and how prominent is it?
How many columns does a page use?
What are the visual elements that are competiting for somebody’s attention?
And what’s going on in the photos?
In our visual design lectures
we talked about strategies for using these tools more effectively;
In this lecture we’re going to look at techniques
for being able to measure whether whether the outcome is the outcome you like.

Here’s another example;
this one’s from Forrest Glick at the Stanford Technology Ventures Program.
They have a regular email list
where they send out links to videos about entrepreneurship that they record.
It’s a great mailing list,
and Forrest was interested in increasing the number of people
who click through this email to see the content that they have on offer.

This is the original version of their email,
and here’s a revised version.
In this case the change is pretty subtle:
The goal is to provide more real estate and emphasis
to the Quick Shots at the top of the email.
This email was sent to a little more than 12,000 people,
about half to each.
And what you see is modest changes
in the number of people who look at the full content —
that is, the email client loaded the external resources —,
the number of people who clicked through one of these links to web content,
and a reasonable significant increase
in the number of people who forwarded this email to somebody else.
What I like about this email example
is it show how you can use these techniques
not just for things that are literally web sites,
but any time that you have online media,
you can get this kind of feedback.
And it also shows how you don’t need to make drastic changes;
you can make relatively small changes and still see significant results.

In 2008, Stanford alumni Dan Siroker left his job at Google
to join the Obama campaign.
And what Dan worked on with the Obama campaign
was improving the effectiveness of their online materials.
After the campaign,
we invited Dan to come back and speak to Stanford students
about the techniques that they were using.
In this video Dan describes a couple of experiments that they did.
Think about of the multiple different versions that you’re going to see:
Which one *you* think would be the most effective —
if you were in the driver’s seat, which one would you pick?

([Voice of Dan Siroker] And here’s an experiment that we did:
This is sort of the layout.
We did a multivariate test,
which is actually just a big complicated word to describe
“trying variations of two different parts of the page.”
We did different kinds of media —
so the top part of the page.
And we tried different kinds of buttons.
A baseline signup rate was 8.26%.
So these are the different buttons we used and the different media.
We had four different buttons:
“Sign Up,” “Learn More,” “Join Us Now,” “Sign Up Now,”
and six different media — three image[s] and three videos.
I’ll show you all of them, and then we’ll do a vote.
And [we’ll] start off and do a bunch and see what wins amongst you guys.

Ok, so this is the first one: The button is “Sign Up.”
“Learn More.”
“Join Us Now.”
And “Sign Up Now.”)

So for these buttons the *only* thing that is changing is the text.
And what they’re going to use as a dependent variable
is how many people clicked that button.

Dan found a pretty significant difference in the signup rates
for these four different small changes in the language of the text.
The baseline rate of the original choice was — for “Sign Up” — was 7.51%.
For “Learn More,” it jumped to 8.91%,
which may not sound like a giant difference,
but that’s an 18.6% increase.
And, over the giant numbers of visitors to a site like this,
that translates into a huge difference
in the amount of revenue you’ve brought in.
“Sign Up Now” performed basically the same as “Sign Up,” 7.52%.
And “Join Us Now” did slightly worse — not meaningfully but slightly.

Dan and his team also explored six different variations
of what the home page might look like —
three different images and three different videos.
Let’s see these six options.

(Ok, [here’s] the baseline images:
“Get Involved,”
“Family,”
“Change.”
And now I’ll show you three of the videos.)

([Media: “Barack’s Video”] Hi, I’m Barack Obama.
Welcome to barackobama.com,
the official website of my campaign for president.)

([Media: “Springfield Video”] Obama! Obama! Obama!

It’s been a long time since I wanted to hear what a politician has to say.

You came here because you believe in what this country can [inaudible].)

([Media: “Sam’s Video”] Well, I say to them tonight:
There is is not a Liberal America
and a Conservative America —
There is the United States of America.)

(The reality is every video did worse than every image on the media.
And you can see actually,
this Springfield Video that actually won amongst you guys
did the worst of all the media.
In terms of the buttons,
“Learn More” did the best by a huge margin.
So if you looked at the specific numbers —
you can see ’em —
“Learn More” did better than the baseline by 18% in terms of actual improvement.
And so this test really validated that, you know, 
in general, these assumptions that you — you know, your gut reaction —
may actually be off [inaudible] in what you’re doing the best.)

So this example from Dan Siroker underscores two important points
about designing in the online world.
The first is that seemingly insignificant changes —
like just the wording on a button — can have huge impacts.
The second one is that our expectations are often wrong —
I think many people would assume that a video might be more compelling,
maybe even if you’re watching this one.
However, in the case of this banner video,
it caused a number of problems
and led to a less compelling experience —
or at least a less compelling clickthrough rate overall —
and that’s one reason why it’s important to check your assumptions.

Dustin Curtis is a user interface designer with a blog.
And he wrote a blog post about his experience
trying out different alternatives for text on his blog
that send people to his Twitter feed.
You can see the four options here.
The first one is “I’m on Twitter.”
Next is “Follow me on Twitter.”
Third is “You should follow me on Twitter.”
And finally, “You should follow me on Twitter here.”
Give some thought to which of these four perform best,
and you can try out the quiz.

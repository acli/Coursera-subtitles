1
00:00:02,081 --> 00:00:05,057
Let's talk about Kneser Ney Smoothing, one
of the most sophisticated forms of smoothing,

2
00:00:05,057 --> 00:00:11,042
but also one with a beautiful
and elegant, intuition. Remember that from

3
00:00:11,042 --> 00:00:17,008
good touring, we talked about the c stars,
the discounted counts you end up from good

4
00:00:17,008 --> 00:00:24,035
touring and we discounted each of the
counts, the count of one was discounted to

5
00:00:24,035 --> 00:00:29,004
point four, and the count of two
discounted to 1.26 and so on, in order to

6
00:00:29,004 --> 00:00:34,024
save mass to replace the zero counts with
some low number. And if you look at the

7
00:00:34,024 --> 00:00:40,051
actual values of these counts, 8.25 for
nine and 7.24 for eight, you'll notice

8
00:00:40,051 --> 00:00:45,018
that in a very large number of cases.
[sound]. The discounted count has a very

9
00:00:45,018 --> 00:00:49,020
close relationship, with the original
count. It's really the original count

10
00:00:49,020 --> 00:00:56,086
minus 0.75, or somewhere close to that.
So. In practice what good touring often

11
00:00:56,086 --> 00:01:03,099
does is produce a fixed small discount
from the count. And that intuition, that,

12
00:01:03,099 --> 00:01:09,005
of a fixed small discount can be applied
directly. When we do this, we call this

13
00:01:09,005 --> 00:01:12,072
absolute discounting, and absolute
discounting is a popular kind of

14
00:01:12,072 --> 00:01:18,039
smoothing. And here we're showing you
absolute discounting interpolation. And

15
00:01:18,039 --> 00:01:23,029
again, the intuition is just we'll save
some time and have to compute all those

16
00:01:23,029 --> 00:01:26,056
complicated good touring numbers and we'll
just subtract.75, or maybe it will be a

17
00:01:26,056 --> 00:01:29,014
different discount value for different
corpora. Now here's the equation for

18
00:01:29,014 --> 00:01:32,099
absolute discounting. So we, we're doing
diagrams again. So the probability.

19
00:01:32,099 --> 00:01:39,031
Absolute discounted of a word, given the
previous word, will be some discounted

20
00:01:39,031 --> 00:01:44,007
bigram interpolated, with some
interpolation weight, with the unigram

21
00:01:44,007 --> 00:01:48,022
probability. So we have a unigram
probability PFW, and then the bigram

22
00:01:48,022 --> 00:01:53,084
probability, and we just subtract a fixed
amount. Let's say it's.75 from the count.

23
00:01:53,084 --> 00:01:57,086
And otherwise, compute the bigram
probability in the normal way. So, we have

24
00:01:57,086 --> 00:02:02,020
a discounted bigram probability, mixed
with some weight, which I'll talk later

25
00:02:02,020 --> 00:02:04,059
about how to set this weight, with a
unigram. And, maybe, we might keep a

26
00:02:04,059 --> 00:02:09,017
couple extra values of D for accounts one
and two. Accounts one and two on a

27
00:02:09,017 --> 00:02:14,000
previous slide weren't quite subtracting
point seven five, so we can model this

28
00:02:14,000 --> 00:02:20,013
more carefully by having separate counts
for those. But the problem with absolute

29
00:02:20,013 --> 00:02:25,056
discounting is the unigram probability
itself. And I want to talk about changing

30
00:02:25,056 --> 00:02:29,074
the unigram probability. And that's the
fundamental intuition of Kneser-Ney. So in

31
00:02:29,074 --> 00:02:34,025
Kneser-Ney smoothing, the idea is keep
that same interpolation that we saw in

32
00:02:34,025 --> 00:02:37,068
absolute discounting, but use a better
estimate of probabilities of the lower

33
00:02:37,068 --> 00:02:42,009
unigrams. And the intuition for that, we
can go back and look at the classic

34
00:02:42,009 --> 00:02:45,065
Shannon games. Remember, in the Shannon
game we're predicting a word from previous

35
00:02:45,065 --> 00:02:50,096
words, so we see a, a sentence, I can't
see without my reading. What's the most

36
00:02:50,096 --> 00:02:54,082
likely next word? Well, glasses seems
pretty likely. Well, how about instead the

37
00:02:54,082 --> 00:03:00,032
word Francisco? Well, that seems very
unlikely in this situation and yet,

38
00:03:00,032 --> 00:03:08,068
Francisco as just a unigram is more common
than glasses. But the reason why Francisco

39
00:03:08,068 --> 00:03:12,093
seems like a bad thing after reading, one
intuition we might be able to get is that

40
00:03:12,093 --> 00:03:18,015
Francisco always follows San or very often
follows San. So while Francisco is very

41
00:03:18,015 --> 00:03:24,012
frequent, it's frequent in the context of
the word San. Now, unigrams in an

42
00:03:24,012 --> 00:03:30,076
interpellation model, where we're mixing a
unigram and a bigram, are specifically

43
00:03:30,076 --> 00:03:33,032
useful, they're, they're very helpful just
in case where we haven't seen a bigram. So

44
00:03:33,032 --> 00:03:35,096
it's unfortunate that just in the case
where we haven't seen a bigram, reading

45
00:03:35,096 --> 00:03:41,005
Francesco, we're trusting Francesco's
unigram weight which is just where we

46
00:03:41,005 --> 00:03:47,041
should trust him. So instead of using the
probability of W, how likely is a word,

47
00:03:47,041 --> 00:03:50,066
our intuition is going to be when we're
backing off to something we should instead

48
00:03:50,066 --> 00:03:53,075
use the continuation probability. We're
going to call it P continuation of a word.

49
00:03:53,075 --> 00:03:58,072
How likely is the word to appear as a
novel continuation? Well, how do we

50
00:03:58,072 --> 00:04:02,078
measure novel continuation? Well, for each
word we'll just count the number of bigram

51
00:04:02,078 --> 00:04:07,082
types it completes. How many different
bigrams does it, does it create by

52
00:04:07,082 --> 00:04:12,012
appearing after another word. In other
words, each bigram type is a novel

53
00:04:12,012 --> 00:04:16,067
continuation the first time we see this
new bigram. In other words, the

54
00:04:16,067 --> 00:04:21,034
continuation probability. Is gonna be
proportional to, the Cardinality of this

55
00:04:21,034 --> 00:04:27,081
set. The number of words of, of preceding
words, I minus one, that occur with our

56
00:04:27,081 --> 00:04:34,029
word. So, how many, whats, how many words,
occur, before this word in a Bi-gram. How

57
00:04:34,029 --> 00:04:38,005
many preceding words are there. That will
be, that, that [inaudible], the

58
00:04:38,005 --> 00:04:42,023
Cardinality of that set, that's a number
we would like our continuation

59
00:04:42,023 --> 00:04:45,030
probability, to be proportional to. So how
many times does W appear as a [inaudible]

60
00:04:45,030 --> 00:04:51,034
continuation? We need to turn that into a
probability. So we just divide by the

61
00:04:51,034 --> 00:04:56,054
total number of word bi-gram types. So, of
all word bi-grams. Them, that occur, more

62
00:04:56,054 --> 00:05:01,060
than zero times, what's the cardinal event
set? How many different word by gram types

63
00:05:01,060 --> 00:05:04,038
are there, and we're just going to divide
the two, to get a probability of

64
00:05:04,038 --> 00:05:09,093
continuation, of all the number of word by
gram types how many of those have W as a

65
00:05:09,093 --> 00:05:14,095
novel continuation? Now it turns out that
there's an alternative metaphor for the

66
00:05:14,095 --> 00:05:21,087
same equations so again we can see the
numerator as the number, the total number

67
00:05:21,087 --> 00:05:27,047
of word types that precede W, how many
word types can W follow And we're gonna

68
00:05:27,047 --> 00:05:31,069
normalize it by the number of words that
could precede all words. So, this sum over

69
00:05:31,069 --> 00:05:39,029
all words of, the, then, number of word
types that can precede the word. And these

70
00:05:39,029 --> 00:05:44,016
two are the same. The number of this
denominator and the denominator we saw on

71
00:05:44,016 --> 00:05:48,092
the previous slide are the same because
the number of possible biogram types is

72
00:05:48,092 --> 00:05:53,087
the same as the number of word type that
can precede all word [inaudible] over

73
00:05:53,087 --> 00:05:57,077
words. If you think about it for a second,
you'll realize that true. So in other

74
00:05:57,077 --> 00:06:04,034
words with this kind of Kneser nine a
frequent word like Francisco that occurs

75
00:06:04,034 --> 00:06:09,019
only in one context like San will have a
low continuation probability. So if we put

76
00:06:09,019 --> 00:06:13,021
together, the intuition of absolute
discounting with the Kneser-Ney

77
00:06:13,021 --> 00:06:19,039
probability for the lower order Ingram, we
have the Kneser-Ney smoothing algorithm.

78
00:06:19,039 --> 00:06:26,047
So, For the bigram itself we just have
absolute discounting. We take the, the

79
00:06:26,047 --> 00:06:30,032
bigram count. We subtract some d discount.
And I've just shown here that we take the

80
00:06:30,032 --> 00:06:34,034
max of that n0 because, obviously, if the
discount happens to be higher than the

81
00:06:34,034 --> 00:06:36,066
probability we don't want a negative
probability. And we're just gonna

82
00:06:36,066 --> 00:06:40,056
interpret like that with this same
continuation probability that we just saw,

83
00:06:40,056 --> 00:06:46,067
p continuation of w sub i. And, the
lambda, now let's talk about how to

84
00:06:46,067 --> 00:06:51,013
compute that lambda. The lambda is gonna
take all that probability mass from all

85
00:06:51,013 --> 00:06:55,089
those, all those normalized discounts that
we took out of these higher-order

86
00:06:55,089 --> 00:07:02,011
probabilities, and w-, use those to, to,
to, to weight, how much probability we

87
00:07:02,011 --> 00:07:05,089
should assign to the unigram. We're gonna
combine those. So that lambda is the, the

88
00:07:05,089 --> 00:07:14,046
amount of the discount weight divided by
the, the, the denominator, there. So, it's

89
00:07:14,046 --> 00:07:19,046
the normalized discount. And then, we're
gonna multiply that by the total number of

90
00:07:19,046 --> 00:07:24,047
word types can follow this context, WI
minus one. In other words, how many

91
00:07:24,047 --> 00:07:29,021
different word types? Did we discount or
how many times did we apply this

92
00:07:29,021 --> 00:07:33,018
normalized discount? And we multiply those
together and we get, we know how much

93
00:07:33,018 --> 00:07:38,040
probably mass total we can, now, assign to
the continuation of the worm. Now, this is

94
00:07:38,040 --> 00:07:43,058
the bigram formulation for [inaudible].
Now in this slide, we're showing you the

95
00:07:43,058 --> 00:07:47,039
general recursive formulation for engrams
in general, and here we have to make a

96
00:07:47,039 --> 00:07:52,022
slight change to, To deal with all the
high-order n-grams. So here we're just

97
00:07:52,022 --> 00:07:56,024
showing the Kneser-Ney probability of a
word, given the prefix of the word. And,

98
00:07:56,024 --> 00:08:01,039
just like Kneser-Ney we saw before, we're
just interpolating a bi, a, a high-order

99
00:08:01,039 --> 00:08:06,052
n-gram which is discounted with, with a, a
lambda weight and a lower-order

100
00:08:06,052 --> 00:08:11,035
probability. But now we need to
distinguish between the very first. Top

101
00:08:11,035 --> 00:08:15,081
level time that we use a count and these
lower order, counts. So we're going to use

102
00:08:15,081 --> 00:08:21,078
the actual count for the very high if
order bi-gram, and we're going to use the

103
00:08:21,078 --> 00:08:25,022
continuation value that we, just defined
earlier for all the lower order

104
00:08:25,026 --> 00:08:30,053
probabilities. So we'll define this new
thing count kinase [inaudible] of dot

105
00:08:30,053 --> 00:08:36,006
which will mean actual count. This will be
actual count, let's say we're doing

106
00:08:36,006 --> 00:08:41,026
trigrams for the trigram. And then when we
recurse and have the,

107
00:08:41,026 --> 00:08:43,070
the Kneser-Ney probability for the lower
order things when we get down to the

108
00:08:43,070 --> 00:08:46,025
bi-grams and uni-grams, we'll be using the
continuation count that's again the, the,

109
00:08:46,025 --> 00:08:51,006
the single word context that we defined
earlier. So Kneser-Ney smoothing,

110
00:08:51,006 --> 00:08:54,040
a very excellent algorithm. It's very
commonly used in speech recognition and

111
00:08:54,040 --> 99:59:59,000
machine translation and yet it has a very
beautiful and elegant intuition and I hope

112
99:59:59,000 --> 99:59:59,000
you appreciate it.

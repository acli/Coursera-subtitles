1
00:00:00,040 --> 00:00:03,099
In this segment I'm going to show you that dependency syntax

2
00:00:03,099 --> 00:00:09,004
is a very natural representation for relation extraction applications.

3
00:00:10,070 --> 00:00:16,050
One domain in which a lot of work has been done on relation extraction is in the biomedical text domain.

4
00:00:16,050 --> 00:00:19,040
So here for example, we have the sentence

5
00:00:19,040 --> 00:00:26,019
“The results demonstrated that KaiC interacts rhythmically with SasA, KaiA, and KaiB.”

6
00:00:26,019 --> 00:00:30,056
And what we’d like to get out of that is a protein interaction event.

7
00:00:30,056 --> 00:00:34,062
So here’s the “interacts” that indicates the relation,

8
00:00:34,062 --> 00:00:36,074
and these are the proteins involved.

9
00:00:36,074 --> 00:00:40,016
And there are a bunch of other proteins involved as well.

10
00:00:40,053 --> 00:00:48,021
Well, the point we get out of here is that if we can have this kind of dependency syntax,

11
00:00:48,021 --> 00:00:55,021
then it's very easy starting from here to follow along the arguments of the subject and the preposition “with”

12
00:00:55,021 --> 00:00:59,036
and to easily see the relation that we’d like to get out.

13
00:00:59,036 --> 00:01:01,071
And if we're just a little bit cleverer,

14
00:01:01,071 --> 00:01:05,081
we can then also follow along the conjunction relations

15
00:01:05,081 --> 00:01:12,096
and see that KaiC is also interacting with these other two proteins.

16
00:01:14,025 --> 00:01:17,036
And that's something that a lot of people have worked on.

17
00:01:17,036 --> 00:01:24,035
In particular, one representation that’s being widely used for relation extraction applications in biomedicine

18
00:01:24,035 --> 00:01:27,079
is the Stanford dependencies representation.

19
00:01:27,079 --> 00:01:33,063
So the basic form of this representation is as a projective dependency tree.

20
00:01:33,063 --> 00:01:40,069
And it was designed that way so it could be easily generated by postprocessing of phrase structure trees.

21
00:01:40,069 --> 00:01:44,007
So if you have a notion of headedness in the phrase structure tree,

22
00:01:44,007 --> 00:01:49,063
the Stanford dependency software provides a set of matching pattern rules

23
00:01:49,063 --> 00:01:55,029
that will then type the dependency relations and give you out a Stanford dependency tree.

24
00:01:55,029 --> 00:02:01,099
But Stanford dependencies can also be, and now increasingly are generated directly

25
00:02:01,099 --> 00:02:06,074
by dependency parsers such as the MaltParser that we looked at recently.

26
00:02:07,031 --> 00:02:11,046
Okay, so this is roughly what the representation looks like.

27
00:02:11,046 --> 00:02:13,029
So it's just as we saw before,

28
00:02:13,029 --> 00:02:17,085
with the words connected by type dependency arcs.

29
00:02:19,065 --> 00:02:24,024
But something that has been explored in the Stanford dependencies framework

30
00:02:24,024 --> 00:02:27,077
is, starting from that basic dependencies representation,

31
00:02:27,077 --> 00:02:34,005
let’s make some changes to it to facilitate relation extraction applications.

32
00:02:34,005 --> 00:02:38,048
And the idea here is to emphasize the relationships

33
00:02:38,048 --> 00:02:43,030
between content words that are useful for relation extraction applications.

34
00:02:43,030 --> 00:02:45,038
Let me give a couple of examples.

35
00:02:45,038 --> 00:02:51,055
So, one example is that commonly you’ll have a content word like “based”

36
00:02:51,055 --> 00:02:56,059
and where the company here is based—Los Angeles—

37
00:02:56,059 --> 00:03:01,002
and it’s separated by this preposition “in”, a function word.

38
00:03:01,002 --> 00:03:07,010
And you can think of these function words as really functioning like case markers in a lot of other languages.

39
00:03:07,010 --> 00:03:11,040
So it’d seem more useful if we directly connected “based” and “LA”,

40
00:03:11,040 --> 00:03:15,003
and we introduced the relationship of “prep_in”.

41
00:03:15,091 --> 00:03:20,073
And so that’s what we do, and we simplify the structure.

42
00:03:20,073 --> 00:03:22,098
But there are some other places, too,

43
00:03:22,098 --> 00:03:29,064
in which we can do a better job at representing the semantics with some modifications of the graph structure.

44
00:03:29,064 --> 00:03:34,086
And so a particular place of that is these coordination relationships.

45
00:03:34,086 --> 00:03:40,039
So we very directly got here that “Bell makes products”.

46
00:03:40,039 --> 00:03:44,015
But we’d also like to get out that Bell distributes products,

47
00:03:44,015 --> 00:03:51,081
and one way we could do that is by recognizing this “and” relationship

48
00:03:51,081 --> 00:04:01,082
and saying “Okay, well that means that ‘Bell’ should also be the subject of ‘distributing’

49
00:04:03,015 --> 00:04:07,049
and what they distribute is ‘products.’”

50
00:04:09,043 --> 00:04:11,031
And similarly down here,

51
00:04:11,031 --> 00:04:21,010
we can recognize that they’re computer products as well as electronic products.

52
00:04:21,078 --> 00:04:24,060
So we can make those changes to the graph,

53
00:04:24,060 --> 00:04:28,011
and get a reduced graph representation.

54
00:04:28,059 --> 00:04:33,048
Now, once you do this, there are some things that are not as simple.

55
00:04:33,048 --> 00:04:38,085
In particular, if you look at this structure, it’s no longer a dependency tree

56
00:04:38,085 --> 00:04:43,001
because we have multiple arcs pointing at this node,

57
00:04:43,001 --> 00:04:46,012
and multiple arcs pointing at this node.

58
00:04:47,025 --> 00:04:48,056
But on the other hand,

59
00:04:48,056 --> 00:04:54,058
the relations that we’d like to extract are represented much more directly.

60
00:04:54,058 --> 00:04:58,000
And let me just show you one graph that gives an indication of this.

61
00:04:58,065 --> 00:05:06,042
So, this was a graph that was originally put together by Jari Björne et al,

62
00:05:06,042 --> 00:05:12,046
who were the team that won the BioNLP 2009 shared tasks in relation extraction

63
00:05:12,046 --> 00:05:17,049
using, as the representational substrate, Stanford dependencies.

64
00:05:17,049 --> 00:05:20,067
And what they wanted to illustrate with this graph

65
00:05:20,067 --> 00:05:25,023
is how much more effective dependency structures were

66
00:05:25,023 --> 00:05:30,085
at linking up the words that you wanted to extract in a relation,

67
00:05:30,085 --> 00:05:34,075
than simply looking for words in the linear context.

68
00:05:35,043 --> 00:05:40,040
So, here what we have is that this is the distance

69
00:05:40,092 --> 00:05:45,089
which can be measured either by just counting words to the left or right,

70
00:05:45,089 --> 00:05:50,004
or by counting the number of dependency arcs that you have to follow.

71
00:05:50,004 --> 00:05:53,032
And this is the percent of time that it occurred.

72
00:05:53,032 --> 00:05:56,033
And so what you see is, if you just look at linear distance,

73
00:05:56,033 --> 00:06:02,089
there are lots of times that there are arguments and relations that you want to connect out

74
00:06:02,089 --> 00:06:06,022
that are four, five, six, seven, eight words away.

75
00:06:06,022 --> 00:06:11,072
In fact, there’s even a pretty large residue here of well over ten percent

76
00:06:11,072 --> 00:06:16,076
where the linear distance away in words is greater than ten words.

77
00:06:16,076 --> 00:06:21,017
If on the other hand though, you are trying to identify,

78
00:06:21,017 --> 00:06:25,063
relate the arguments of relations by looking at the dependency distance,

79
00:06:25,063 --> 00:06:30,045
then what you’d discover is that the vast majority of the arguments

80
00:06:30,045 --> 00:06:35,042
are very close-by neighbors in terms of dependency distance.

81
00:06:35,042 --> 00:06:42,006
So, about 47 percent of them are direct dependencies and another 30 percent of distance too.

82
00:06:42,006 --> 00:06:48,051
So take those together and that’s greater than three quarters of the dependencies that you want to find.

83
00:06:48,051 --> 00:06:51,053
And then this number trails away quickly.

84
00:06:51,053 --> 00:06:59,043
So there are virtually no arguments of relations that aren’t fairly close together in dependency distance

85
00:06:59,043 --> 00:07:02,062
and it’s precisely because of this reason that you can get

86
00:07:02,062 --> 00:07:09,061
a lot of mileage in doing relation extraction by having a representation-like dependency syntax.

87
00:07:11,044 --> 00:07:16,004
Okay, I hope that’s given you some idea of why knowing about syntax is useful,

88
00:07:16,004 --> 99:59:59,000
when you want to do various semantic tasks in natural language processing.

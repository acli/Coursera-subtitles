1
00:00:00,000 --> 00:00:05,088
In this segment I'm going to show you,
that dependency syntax is a very natural

2
00:00:05,088 --> 00:00:12,027
representation, for relation extraction
applications. One domain of which a lot of

3
00:00:12,027 --> 00:00:12,027
work has been done on relation extraction
is in the biomedical text domain. So here

4
00:00:12,027 --> 00:00:12,027
for example, we have the sentence that the
results demonstrated that KC interacts

5
00:00:12,027 --> 00:00:12,027
rhythmically with [inaudible], KA and KB.
And what we?d like to get out of that is a

6
00:00:12,027 --> 00:00:12,027
protein interaction event. So here's the
interacts that is sort of indicates the

7
00:00:12,027 --> 00:00:12,027
relation and these are the proteins
involved. And there are a bunch of other

8
00:00:12,027 --> 00:00:12,027
proteins as well. Well, the point we get
out of here, is that if we can, have this

9
00:00:12,027 --> 00:00:12,027
kind of dependency syntax. And it's very
easy starting from here to follow along

10
00:00:12,027 --> 00:00:12,027
the arguments of the subject and the
preposition with and to easily see the

11
00:00:12,027 --> 00:00:12,027
relation that we'd like to get out, and if
we're just a little bit cleverer, we can

12
00:00:12,027 --> 00:00:12,027
then also follow along the conjunction
relations and see that KC is also

13
00:00:12,027 --> 00:00:12,027
interacting with these other two proteins.
And that's something that a lot of people

14
00:00:12,027 --> 00:00:12,027
have worked on. In particular, one
representation that's being widely used

15
00:00:12,027 --> 00:00:12,027
for relation extraction applications in
biomedicine is the Stanford dependencies

16
00:00:12,027 --> 00:00:12,027
representation. So the basic form of this
representation is as a projective

17
00:00:12,027 --> 00:00:12,027
dependency tree. And it was designed that
way so it could be easily generated by

18
00:00:12,027 --> 00:00:12,027
post processing of phrase structure trees.
So if you have a notice of hiddenness in

19
00:00:12,027 --> 00:00:12,027
the phrase structure tree, the Stanford
dependency software provides a set of

20
00:00:12,027 --> 00:00:12,027
matching pattern rules that will then type
the dependency relations and give you out

21
00:00:12,027 --> 00:00:12,027
a Stanford dependency tree. But Stanford
dependencies can also be a now

22
00:00:12,027 --> 00:00:12,027
increasingly odd generated, generated
directly by dependency [inaudible], such

23
00:00:12,027 --> 00:00:12,027
as the [inaudible] that we looked at
recently. Okay so this is roughly, what

24
00:00:12,027 --> 00:00:00,000
the representation looks like. So it's
just as we saw before, with the words

25
00:00:00,000 --> 00:00:00,000
connected by type dependency arcs. But
something that has been explored in the

26
00:00:00,000 --> 00:00:00,000
Stanford dependencies framework, is,
starting fro m that basic dependencies

27
00:00:00,000 --> 00:00:00,000
representation. Let's make some changes to
it to facilitate relation extraction

28
00:00:00,000 --> 00:00:00,000
applications. And the idea here is to
emphasize the relationships between

29
00:00:00,000 --> 00:00:00,000
content words that are useful for relation
extraction applications. Let me give a

30
00:00:00,000 --> 00:00:00,000
couple of examples. So, one example is
that. Commonly you'll have a content word

31
00:00:00,000 --> 00:00:00,000
like based, and where. The company here is
based Los Angeles, and it's separated by

32
00:00:00,000 --> 00:00:00,000
this preposition in, or function word, and
you can think of these function words as

33
00:00:00,000 --> 00:00:00,000
really functioning like case markers in a
lot of other languages. So it's seen more

34
00:00:00,000 --> 00:00:00,000
useful if we directly connect it based LA,
and we introduce the relationship of prep

35
00:00:00,000 --> 00:00:00,000
in. And so, that's what we do, and we
simplify the structure. But there are some

36
00:00:00,000 --> 00:00:00,000
other places, too, in which we can do a
better job at representing the semantics

37
00:00:00,000 --> 00:00:00,000
with some modifications of the graph
structure, and so a particular place of

38
00:00:00,000 --> 00:00:00,000
that is these coordination relationships.
So we very directly got here that. Bell

39
00:00:00,000 --> 00:00:00,000
makes products, but we'd also like to get
out that Bell distributes products, and

40
00:00:00,000 --> 00:00:00,000
one way we could do that, is by,
recognizing, this and relationship and

41
00:00:00,000 --> 00:00:00,000
saying Okay, well that means that Bell,
should also, be the subject of

42
00:00:00,000 --> 00:00:00,000
distributing. And what they distribute, is
products. [sound]. And similarly down

43
00:00:00,000 --> 00:00:00,000
here, we can recognize, that they're,
they're computer products, as well as

44
00:00:00,000 --> 00:00:00,000
electronic products. So we can make those
changes, to, to the graph, and get a kind

45
00:00:00,000 --> 00:00:00,000
of reduced graph representation. Now, once
you do this, there are some things that

46
00:00:00,000 --> 00:00:00,000
are not as simple. In particular, if you
look at this structure, it's no longer a

47
00:00:00,000 --> 00:00:00,000
dependency tree. Because we have multiple
arcs pointing at this node, and multiple

48
00:00:00,000 --> 00:00:00,000
arcs pointing at this node. But on the
other hand, the relations that we'd like

49
00:00:00,000 --> 00:00:00,000
to extract are represented much more
directly. And let me just show you one

50
00:00:00,000 --> 00:00:00,000
graft that gives an indication of this.
So, this was a graph that was. Originally

51
00:00:00,000 --> 00:00:00,000
put together by [inaudible] et al, who
were the team that won the Bio NLP 2009

52
00:00:00,000 --> 00:00:00,000
shared l relation extraction, using as the
representational substrate, Stanford

53
00:00:00,000 --> 00:00:00,000
dependencies. And what they wanted to
illustrate with this graph is how much

54
00:00:00,000 --> 00:00:00,000
more effective dependency structures were
at linking up the words that you wanted to

55
00:00:00,000 --> 00:00:00,000
extract in a relation, than simply looking
for words in the linear context. So, here

56
00:00:00,000 --> 00:00:00,000
what we have is that this is the distance
which can be measured either by just

57
00:00:00,000 --> 00:00:00,000
counting words to the left or right. Or by
counting the number of dependency arcs

58
00:00:00,000 --> 00:00:00,000
that you have to follow. And this is the
percent of time that it occurred. And so

59
00:00:00,000 --> 00:00:00,000
what you see is, if you just look at
linear distance. There are lots of times

60
00:00:00,000 --> 00:00:00,000
that there are arguments and relations
that you want to connect out that are

61
00:00:00,000 --> 00:00:00,000
four, five, six, seven, eight words away.
In fact, there's even a pretty large

62
00:00:00,000 --> 00:00:00,000
residue here of well over ten percent
where the linear distance away in words is

63
00:00:00,000 --> 00:00:00,000
greater than ten words. If on the other
hand though, you, I try and identify,

64
00:00:00,000 --> 00:00:00,000
relate the arguments of relations by
looking at the dependency distance, then

65
00:00:00,000 --> 00:00:00,000
what you discover is that the vast
majority of the arguments are very close

66
00:00:00,000 --> 00:00:00,000
by neighbors in terms of dependency
distance. So that 47 percent of them are

67
00:00:00,000 --> 00:00:00,000
direct dependencies and another 30 percent
of distance too. So take those together

68
00:00:00,000 --> 00:00:00,000
and that's greater than three quarters of
the dependencies that you want to find.

69
00:00:00,000 --> 00:00:00,000
And then this number trails away quickly.
So there are virtually no. Arguments of

70
00:00:00,000 --> 00:00:00,000
relations that aren't fairly close
together in dependency distance and it's

71
00:00:00,000 --> 00:00:00,000
precisely because of this reason that, you
can get a lot of mileage, in, doing

72
00:00:00,000 --> 00:00:00,000
relation extraction by having a
representation like dependency syntax.

73
00:00:00,000 --> 00:00:00,000
Okay, I hope that?s given you some idea of
why knowing about syntax is useful, when

74
00:00:00,000 --> 00:00:00,000
you want to do various semantic tasks in
natural language processing.

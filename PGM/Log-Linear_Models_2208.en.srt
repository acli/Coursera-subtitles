1
00:00:00,000 --> 00:00:05,043
Local structure that doesn’t require full
table representations

2
00:00:05,045 --> 00:00:09,007
is important in both directed and undirected models.

3
00:00:09,007 --> 00:00:14,086
How do we incorporate local structure into undirected models?

4
00:00:14,086 --> 00:00:22,008
The framework for that is called “log-linear models” for reasons that will be clear in just a moment.

5
00:00:23,008 --> 00:00:24,022
So

6
00:00:26,056 --> 00:00:33,014
Whereas, in the original representation of the unnormalized density

7
00:00:33,014 --> 00:00:39,018
we defined P tilde as the product of factors Φi(Di),

8
00:00:39,021 --> 00:00:43,068
each [of] which is potentially a full table,

9
00:00:43,073 --> 00:00:49,097
now we're going to shift that representation to something that uses a linear form.

10
00:00:50,000 --> 00:00:54,089
So here's a linear form that is subsequently exponentiated,

11
00:00:54,092 --> 00:00:59,078
and that's why it's called log linear—because the logarithm is a linear function.

12
00:00:59,083 --> 00:01:02,074
So what is this form over here?

13
00:01:02,074 --> 00:01:06,098
It's a linear function that has these things that are called coefficients.

14
00:01:10,017 --> 00:01:12,063
And these things that are called features.

15
00:01:14,020 --> 00:01:22,010
Features, like factors, each have a scope which is a set of variables on which the feature depends.

16
00:01:24,015 --> 00:01:27,002
But different features can have the same scopes.

17
00:01:27,002 --> 00:01:31,017
You can have multiple features all of which are over the same set of variables.

18
00:01:31,017 --> 00:01:36,047
Notice that each feature has just a single parameter wj that multiplies it.

19
00:01:37,065 --> 00:01:40,031
So, what does this give rise to?

20
00:01:40,031 --> 00:01:43,045
I mean if we have a log linear model,

21
00:01:43,048 --> 00:01:49,046
we can push in the exponent through the summation,

22
00:01:49,046 --> 00:01:58,076
and that gives us something that is a product of exponential functions,

23
00:01:58,079 --> 00:02:03,053
so you can think of each of these as effectively
a little factor,

24
00:02:03,053 --> 00:02:07,068
but it’s a factor that only has a single parameter wj.

25
00:02:07,068 --> 00:02:11,020
Since this is a little bit abstract, so let’s look at an example.

26
00:02:11,020 --> 00:02:17,069
Specifically lets look at how we might represent a simple table factor as a log linear model.

27
00:02:17,070 --> 00:02:24,028
So here’s a param, here’s a factor Φ, over two binary random variables X1 and X2.

28
00:02:24,028 --> 00:02:30,078
And so a full table factor would have four parameters, A00, A01, A10, and A11.

29
00:02:30,078 --> 00:02:34,055
So we can capture this model using a log linear model,

30
00:02:34,055 --> 00:02:41,036
using a set of such of features, using a set of,
these guys, which are indicator functions.

31
00:02:41,036 --> 00:02:43,022
So this is an indicator function.

32
00:02:43,022 --> 00:02:48,096
It takes one if X1 is zero and X2 is zero,

33
00:02:48,097 --> 00:02:51,003
and it takes zero otherwise.

34
00:02:51,003 --> 00:02:54,009
So this the general notion of an indicator function.

35
00:02:54,012 --> 00:03:00,031
It looks at the event—or constraint—inside the curly braces,

36
00:03:00,040 --> 00:03:05,072
and it returns a value of 0 or 1, depending on
whether that event is true or not.

37
00:03:05,072 --> 00:03:13,059
And so, if we wanted to represent this factor as a log-linear model,

38
00:03:13,062 --> 00:03:19,092
We can see that we can simply sum up over all the four values of K and L,

39
00:03:19,092 --> 00:03:22,069
which are either 0 or 1, each of them.

40
00:03:22,070 --> 00:03:26,011
So were summing up over all four entries here.

41
00:03:26,012 --> 00:03:32,047
And we have a parameter—or coefficient—w_kl which multiplies this feature.

42
00:03:33,029 --> 00:03:44,012
And so, we would have a summation of w_kl:

43
00:03:44,013 --> 00:03:49,022
of w00 only in the case where X1 is zero and X2 is zero.

44
00:03:49,025 --> 00:04:01,009
So we would have x of negative w00 when X1=0 and X2=0,

45
00:04:01,009 --> 00:04:12,039
and we would have x of negative w01 when
X1=0 and X2=1, and so on and so forth.

46
00:04:12,039 --> 00:04:15,050
And it’s not difficult to convince ourselves that

47
00:04:15,052 --> 00:04:24,077
if we define w_kl to be the negative log of the entries of the
corresponding entries in this table,

48
00:04:24,080 --> 00:04:29,008
then that gives us right back the factor that
we defined to begin with.

49
00:04:29,011 --> 00:04:33,074
So this shows that this is a general representation,

50
00:04:33,075 --> 00:04:40,052
In the sense that we can take any factor and represent it as a log-linear model

51
00:04:40,054 --> 00:04:47,017
simply by including all of the appropriate features.

52
00:04:47,019 --> 00:04:52,035
But we don’t generally want to do that.

53
00:04:52,035 --> 00:04:55,080
Generally we want a much finer grain set of features.

54
00:04:55,080 --> 00:05:01,019
So let’s look at some of the examples of features that people use in practice.

55
00:05:01,019 --> 00:05:03,083
So here are the features used in a language model.

56
00:05:03,085 --> 00:05:07,093
This is a language model that we that we discussed previously.

57
00:05:07,094 --> 00:05:12,027
And here we have features that relate:

58
00:05:12,029 --> 00:05:15,083
First of all, let’s just remind ourselves [that] we have two sets of variables.

59
00:05:15,083 --> 00:05:23,068
We have the variables Y which represent the annotations for each word

60
00:05:23,069 --> 00:05:29,095
in the sequence corresponding to what category that corresponds to.

61
00:05:29,095 --> 00:05:32,042
So this is a person.

62
00:05:32,042 --> 00:05:34,090
This is the beginning of a person name.

63
00:05:34,093 --> 00:05:37,084
This is the continuation of a person name.

64
00:05:37,084 --> 00:05:39,030
The beginning of a location.

65
00:05:39,030 --> 00:05:42,023
The continuation of a location, and so on.

66
00:05:42,023 --> 00:05:48,062
As well as a bunch of words that are not—none of person, location, organization.

67
00:05:48,065 --> 00:05:50,076
And they’re all labeled “other”.

68
00:05:50,078 --> 00:05:55,007
And so the value Y tells us for each word what
category it belongs to,

69
00:05:55,009 --> 00:05:59,037
so that we’re trying to identify people, locations, and
organizations in the sentence.

70
00:05:59,037 --> 00:06:09,019
We have another set of variables X, which are the
actual words in the sentence.

71
00:06:09,022 --> 00:06:12,080
Now we can go ahead and define…

72
00:06:12,080 --> 00:06:16,038
We can use a full table representation that

73
00:06:16,038 --> 00:06:22,072
basically tries to relate each and every Y that has a feature that has a full factor,

74
00:06:22,072 --> 00:06:28,006
that looks at every possible word
in the English language,

75
00:06:28,007 --> 00:06:34,007
but those are going to be very, very, expensive, and a
very large number of parameters.

76
00:06:34,011 --> 00:06:37,083
And so we're going to define a feature that looks, for example,

77
00:06:37,083 --> 00:06:45,020
at f of, say, a particular Y_i, which is the label for the i’th word in the sentence.

78
00:06:45,020 --> 00:06:54,083
And X_i being that i’th word, and that feature says, for
example: Y_i equals person.

79
00:06:54,085 --> 00:07:03,074
It’s the indicator function for Y_i equals person,
and X_i is capitalized.

80
00:07:05,082 --> 00:07:09,069
And so that feature doesn’t look at the individual words.

81
00:07:09,069 --> 00:07:13,019
It just looks at whether that word is capitalized.

82
00:07:13,019 --> 00:07:17,079
Now we have just the single parameter that looks just at capitalization,

83
00:07:17,081 --> 00:07:23,014
and parameterizes how important is capitalization for recognizing that something's a person.

84
00:07:23,015 --> 00:07:26,094
We could also have another feature.

85
00:07:26,094 --> 00:07:32,058
This is an alternative: This a different feature that can and could be part of the same model

86
00:07:32,058 --> 00:07:38,038
that says: Y_i is equal to location,

87
00:07:38,038 --> 00:07:41,006
Or, actually, I was little bit imprecise here—

88
00:07:41,006 --> 00:07:45,046
This might be beginning of person. This might be beginning of location.

89
00:07:45,047 --> 00:07:51,041
And X_i appears in some atlas.

90
00:07:51,042 --> 00:07:55,036
Now there is other things that appear in the other atlas than locations,

91
00:07:55,036 --> 00:07:58,064
but if a word appears in the atlas,

92
00:07:58,066 --> 00:08:01,097
there is a much higher probability presumably that it’s actually a location

93
00:08:02,000 --> 00:08:06,019
and so we might have, again, [a] weight for this feature

94
00:08:06,019 --> 00:08:13,061
that indicates that maybe increases the probability in Y_i being labeled this way.

95
00:08:13,061 --> 00:08:19,043
And so you can imagine that constructing a very rich set of features,

96
00:08:19,043 --> 00:08:24,016
all of which look at certain aspects of the word,

97
00:08:24,016 --> 00:08:31,093
and rather than enumerating all the possible words
and giving a parameter to each and one of them.

98
00:08:31,093 --> 00:08:38,007
Let’s look at some other examples of feature-based models.

99
00:08:38,011 --> 00:08:41,015
So this is the example from statistical physics.

100
00:08:41,015 --> 00:08:43,044
It’s called the Ising model.

101
00:08:43,044 --> 00:08:53,012
And the Ising model is something that looks at pairs
of variable. It’s a pairwis Markov [inaudivle].

102
00:08:53,012 --> 00:08:58,021
And looks the pairs of adjacent variables, and basically
gives us a coefficient for their products.

103
00:08:58,021 --> 00:09:03,050
So now, this is the case where
variables are in the end are binary, but

104
00:09:03,050 --> 00:09:08,086
not in the space zero one that rather
negative one of and positive one. And so o

105
00:09:08,086 --> 00:09:14,049
now, we have a model that's [inaudible] as
features that are just the product of the

106
00:09:14,049 --> 00:09:19,091
values of the adjacent variables. Where
might this come up? It comes up in context

107
00:09:19,091 --> 00:09:25,023
for example, of modeling the spin of
electrons in a grid. So here you have a

108
00:09:25,023 --> 00:09:31,074
case where the electrons can rotate either
along one direction or in the other

109
00:09:31,074 --> 00:09:38,098
direction so here is a bunch of blo, the,
the, the atoms that are marked with a blue

110
00:09:38,098 --> 00:09:45,057
arrow. You have one rotational axe season,
the red arrow rotating in the opposite

111
00:09:45,057 --> 00:09:53,015
direction. And you, and this basically
says we have a term that, who, probability

112
00:09:53,015 --> 00:10:01,095
distribution over the joint set of spin.
So this is the joint, the joint. Spins.

113
00:10:01,095 --> 00:10:08,014
And the model, depends on whether adjacent
atoms have the same spin or opposite spin.

114
00:10:08,014 --> 00:10:13,089
So notice that one times one is the same
as -one times -one. So this really just

115
00:10:13,089 --> 00:10:19,041
looks at whether they have the same spin
or different spins. And there is a

116
00:10:19,041 --> 00:10:25,088
parameter that looks at, you know, same or
different. That's what this feature

117
00:10:25,088 --> 00:10:32,079
represents. And depending on the value of
this parameter over here, if the parameter

118
00:10:32,079 --> 00:10:38,089
goes one way, we're going to favor systems
that, where the atoms spin in the same

119
00:10:38,089 --> 00:10:44,040
direction. And if its going in the
opposite direction, you're going to favor

120
00:10:44,040 --> 00:10:50,028
atoms that spin in a different direction.
And those are called ferromagnetic and

121
00:10:50,028 --> 00:10:58,033
anti-ferromagnetic. Furthermore, you can
define in these systems an ocean of a

122
00:10:58,033 --> 00:11:06,090
temperature. So the temperature here says
how strong is this connection? So notice

123
00:11:06,090 --> 00:11:16,054
that as T grows, as the temperature grows,
the WIJs get divided by T. And the all

124
00:11:16,054 --> 00:11:24,081
kind of go towards zero. Which means that
the strength of the connection between

125
00:11:24,081 --> 00:11:30,098
adjacent atoms, effectively becomes almost
moot, and they become almost a couple from

126
00:11:30,098 --> 00:11:39,013
each other. On the other hand as the
temperature decreases. Then the effect of

127
00:11:39,013 --> 00:11:43,046
the correl-, of the interaction between
the atoms becomes much more signifigant.

128
00:11:43,046 --> 00:11:47,083
And they're going to impose much stronger
constraints on each other. And this is

129
00:11:47,083 --> 00:11:51,094
actually a model of a real physical
system. I mean, this is real temperature,

130
00:11:51,094 --> 00:11:56,004
and real atoms, and so on. And sure
enough, if you look at what happens, to

131
00:11:56,004 --> 00:12:00,063
these models as a function of temperature,
what we see over here is high temperature.

132
00:12:02,027 --> 00:12:08,071
This is high temperature and you can see
that there is a lot, you know, there's a

133
00:12:08,071 --> 00:12:15,013
lot of mixing between the two types of
spin and this is low temperature and you

134
00:12:15,013 --> 00:12:21,016
can see that there's much stronger
constraints in this configuration about

135
00:12:21,016 --> 00:12:30,098
the spins of adjacent atoms. Another kind
of feature that's used very much in, lots

136
00:12:30,098 --> 00:12:36,069
of practical applications is the notion of
a metric, of a metric feature, an M.R.S.

137
00:12:36,069 --> 00:12:41,079
So what's a metric feature? This is
something that comes up, mostly in cases

138
00:12:41,079 --> 00:12:47,026
where you have a bunch of random variables
at. That all take values in some joint

139
00:12:47,026 --> 00:12:52,053
labels, so for example they might all be
[inaudible]. They all might take values

140
00:12:52,053 --> 00:12:58,026
one, two, three, four. And what we'd like
to do is [cough] we have, when we have x I

141
00:12:58,026 --> 00:13:03,086
that are connected to each other, x I and
x j that are connected to each other by an

142
00:13:03,086 --> 00:13:10,009
edge. We want x I and xj to take. Values.
So in order to enforce the fact that X

143
00:13:10,009 --> 00:13:16,006
[inaudible] take similar values we need an
ocean of similarity. And we're going to

144
00:13:16,006 --> 00:13:22,032
encode that using the distance function U,
that takes two values, one for XI and one

145
00:13:22,032 --> 00:13:27,078
for XJ, and says, how close are they to
each other? So what does the distance

146
00:13:27,078 --> 00:13:33,061
function need to be. Well, the distance
function needs to satisfy the standard

147
00:13:33,061 --> 00:13:39,068
condition on a distance function or a
metric so first is reflexivity. Which

148
00:13:39,068 --> 00:13:46,012
means that if the two, if the two
variables take on the same value, then

149
00:13:46,012 --> 00:13:53,073
that distance better be zero. Oh I forgot
to say that this, sorry, this needs to be

150
00:13:53,073 --> 00:14:00,073
a non-negative function. Symmetry means
that, That the distances are symetrical.

151
00:14:00,073 --> 00:14:06,089
So the distance between two values D1 and
D2 are the same as the distance between D2

152
00:14:06,089 --> 00:14:12,032
and D1. And finally, is the triangle
inequality, which says that the distance

153
00:14:12,032 --> 00:14:18,012
betwen D1 and D2, so here is D1 here is
D2, and the distance between D one and D2

154
00:14:18,012 --> 00:14:23,099
is less than the distance between D1 and
D3 and then going to D2. So the standard

155
00:14:23,099 --> 00:14:31,011
triangle inequality. If a, if a distance
just satisfies these two conditions, it's

156
00:14:31,011 --> 00:14:37,041
called a semi metric. Otherwise, if it
satisfies all three, it's called a metric.

157
00:14:37,041 --> 00:14:45,086
And both are actually used in practical
applications. So how do we take this,

158
00:14:46,012 --> 00:14:53,079
distance feature and put it in the context
of an MRF? We have a feature that looks at

159
00:14:53,079 --> 00:15:00,067
two variables, XI and XJ. And that feature
is the distance between XI and XJ. And

160
00:15:00,067 --> 00:15:07,076
now, we put it together by multiplying
that with a coefficient. W. I. J., such

161
00:15:07,076 --> 00:15:17,004
that, W.I.J. Has to be greater than zero.
So that we want, the metric M.R.S. So

162
00:15:17,004 --> 00:15:28,059
what, the effect that, that has is that,
the lower. The distance. The higher. This

163
00:15:28,059 --> 00:15:37,000
is, because of the negative coefficient,
which means that higher the probability.

164
00:15:37,000 --> 00:15:42,018
Okay? So. The, the more pairs you have
that are close to each other and the

165
00:15:42,018 --> 00:15:46,044
closer they are to each other the higher
the probability of the overall

166
00:15:46,044 --> 00:15:53,028
configuration. Which is exactly what we
wanted to have happen. So conversely if

167
00:15:53,028 --> 00:15:58,065
you have very, values that are far from
each other in the distance metric the

168
00:15:58,065 --> 00:16:05,030
lower the probability in the model. Here
are some examples of metric mrf. One the

169
00:16:05,030 --> 00:16:12,004
simplest metric mrf is one that gives the
distance of zero when the two classes are

170
00:16:12,004 --> 00:16:18,062
equal to each other and the distance of
one every where else. You know just like a

171
00:16:18,062 --> 00:16:24,079
step function. This gives rise to a
potential that looks like this. So we have

172
00:16:24,079 --> 00:16:30,026
0's on. The diagonal. So we get a bump in
probability when the two adjacent

173
00:16:30,026 --> 00:16:35,032
variables take on the same label and
otherwise we get a reduction in

174
00:16:35,032 --> 00:16:41,026
probability. But it, but it doesn't matter
what particular value they take. That's

175
00:16:41,026 --> 00:16:47,085
one example of a simple, metric. A
somewhat, more expressive example might

176
00:16:47,085 --> 00:16:54,045
come up when the values V are actually
numerical values. In which case, you can

177
00:16:54,045 --> 00:17:01,063
look at maybe the, difference between the
miracle values. So, VK minus VL. And you

178
00:17:01,063 --> 00:17:07,070
want, and when VK is equal to VL. The
distance is zero, and then you have a

179
00:17:07,070 --> 00:17:13,068
linear, function that increases the
distance as the distance between V.K. And

180
00:17:13,068 --> 00:17:20,067
V.L. Grows. So, this is the absolute
value. A VK minus the L. A more

181
00:17:20,067 --> 00:17:27,004
interesting notion that comes up a lot in
practice is, we don't want to penalize

182
00:17:27,004 --> 00:17:33,063
arbitrarily things that are far way from
each other in label space. So this is what

183
00:17:33,063 --> 00:17:39,082
is called as truncated. When you are
penalty and you can see that beyond a

184
00:17:39,082 --> 00:17:48,040
certain thresh hold. There, the penalty
just becomes constant, so it plateaus. So

185
00:17:48,040 --> 00:17:53,069
that there is a penalty, but it doesn't
keep increasing over, as the labels get

186
00:17:53,069 --> 00:17:59,030
further from each other. One example where
metric MRFs are used, is, when we're doing

187
00:17:59,030 --> 00:18:04,013
image segmentation. And here, we tend to
favor segmentations where adjacent

188
00:18:04,013 --> 00:18:12,071
superpixels. So these are adjacent
superpixels. And we want them to take the

189
00:18:12,071 --> 00:18:22,075
same class. And so here we have no penalty
when the super pixels take the same class

190
00:18:22,075 --> 00:18:28,001
and we have some penalty when they take
different classes. And this is actually a

191
00:18:28,001 --> 00:18:33,009
very common, albeit simple, model for
image segmentation. Let's look at a

192
00:18:33,009 --> 00:18:38,050
different MRF, also in the context of, of
computer vision. This is and MR-, an MRF

193
00:18:38,050 --> 00:18:43,077
that's used for image denoising. So here
we have a noisy version of a real image

194
00:18:43,077 --> 00:18:49,077
that looks like this. So this is, you can
see this kind of, white noise overlayed on

195
00:18:49,077 --> 00:18:54,072
top of the image. And what we'd like to
do, is we'd like to get a cleaned up

196
00:18:54,072 --> 00:18:59,099
version of the image. So here we have, a
set of variables, X, that correspond to

197
00:18:59,099 --> 00:19:06,093
the noisy pixels. And we have a set of
variables, 'y', that corresponds to the

198
00:19:06,093 --> 00:19:14,041
clean pixels. And we'd like to have a
probabilistic model that relates, X and, X

199
00:19:14,041 --> 00:19:20,052
and Y. And what we're going to is we'd
like, so, in [inaudible], I mean,

200
00:19:20,052 --> 00:19:27,062
[inaudible] the, the, so you'd like to
have two effects on the pixels Y. First,

201
00:19:27,062 --> 00:19:34,039
you'd like YI to be close, [inaudible], my
I to be close to XI. But if you just do

202
00:19:34,039 --> 00:19:39,088
that, then you're just going to stick with
the original image. So what is the main

203
00:19:39,088 --> 00:19:45,057
constraint that we can employ on the image
in order to clean it up is the fact that

204
00:19:45,057 --> 00:19:50,099
adjacent pixels tend to have the same
value. So in this case what we're going to

205
00:19:50,099 --> 00:19:57,016
do is we're going to model we're going to
constrain the image so that we're going to

206
00:19:57,016 --> 00:20:03,091
constrain the Yi's to try and make Yi
close to its neighbors. And the further

207
00:20:03,091 --> 00:20:14,086
away it is, the bigger the penalty. And
that's the metric MRF. Now we could use

208
00:20:14,086 --> 00:20:22,035
Just the linear penalty. But, that's going
to be a very fragile Model, because, not

209
00:20:22,035 --> 00:20:28,020
obviously the right answer isn't the model
where all pixels are equal to each other

210
00:20:28,020 --> 00:20:33,050
in their actual intensity value because
that would be just a single grayish

211
00:20:33,050 --> 00:20:38,024
looking image. So when you look
[inaudible], one pixel be part of its

212
00:20:38,024 --> 00:20:44,009
adjacent pixel it's getting [inaudible] in
a different direction either by it's own

213
00:20:44,009 --> 00:20:49,088
observation or by other adjacent pixels.
So the right model to use here is actually

214
00:20:49,088 --> 00:20:54,097
the [inaudible] model and that one is
commonly use for using [inaudible].

215
00:20:54,097 --> 00:21:01,084
Interesting, almost exactly the same idea
is used in the complex of stereo

216
00:21:01,084 --> 00:21:07,088
reconstruction. There, The values that you
like to infer, the Yi's, are the depth

217
00:21:07,088 --> 00:21:13,007
disparity for a given pixel in the image,
how deep it is. And here also we have

218
00:21:13,007 --> 00:21:18,048
spacial continuity. We like the depth of
one pixel to be close to the depth of an

219
00:21:18,048 --> 00:21:23,017
adjacent pixel. But once again we don't
want to enforce this too strongly because

220
00:21:23,017 --> 00:21:27,081
you do have [inaudible] dexterity in the
image and so eventually you'd like things

221
00:21:27,081 --> 00:21:32,007
to be allowed to break away from each
other. And so once again, one typically

222
00:21:32,007 --> 00:21:37,034
uses some kind of truncated, linear model.
For doing the [inaudible] construction,

223
00:21:37,034 --> 00:21:43,091
often [inaudible] by other little tricks.
So, for example, here we have the actual

224
00:21:43,091 --> 00:21:49,071
pixel appearance, for example, the color
and texture are very similar to each

225
00:21:49,071 --> 00:21:55,099
other, you might want to have the stronger
constraint similarity. Versus if the color

226
00:21:55,099 --> 00:22:00,079
and texture of the adjacent pixels are
very different from each other they may be

227
00:22:00,079 --> 00:22:05,059
more likely to belong to different objects
and you don't want to enforce quite as

228
00:22:05,059 --> 00:22:07,057
strong of a similarity constraint.
